<!DOCTYPE html>
<html lang="zh_CN"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>利用Flume将MySQL表数据准实时抽取到HDFS | Java面试</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="利用Flume将MySQL表数据准实时抽取到HDFS" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="利用Flume将MySQL表数据准实时抽取到HDFS 一、为什么要用到Flume 在以前搭建HAWQ数据仓库实验环境时，我使用Sqoop抽取从MySQL数据库增量抽取数据到HDFS，然后用HAWQ的外部表进行访问。这种方式只需要很少量的配置即可完成数据抽取任务，但缺点同样明显，那就是实时性。Sqoop使用MapReduce读写数据，而MapReduce是为了批处理场景设计的，目标是大吞吐量，并不太关心低延时问题。就像实验中所做的，每天定时增量抽取数据一次。 Flume是一个海量日志采集、聚合和传输的系统，支持在日志系统中定制各类数据发送方，用于收集数据。同时，Flume提供对数据进行简单处理，并写到各种数据接受方的能力。Flume以流方式处理数据，可作为代理持续运行。当新的数据可用时，Flume能够立即获取数据并输出至目标，这样就可以在很大程度上解决实时性问题。 Flume是最初只是一个日志收集器，但随着flume-ng-sql-source插件的出现，使得Flume从关系数据库采集数据成为可能。下面简单介绍Flume，并详细说明如何配置Flume将MySQL表数据准实时抽取到HDFS。 二、Flume简介 1. Flume的概念 Flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到HDFS，简单来说flume就是收集日志的，其架构如图1所示。 图1 2. Event的概念 在这里有必要先介绍一下Flume中event的相关概念：Flume的核心是把数据从数据源（source）收集过来，在将收集到的数据送到指定的目的地（sink）。为了保证输送的过程一定成功，在送到目的地（sink）之前，会先缓存数据（channel）,待数据真正到达目的地（sink）后，Flume再删除自己缓存的数据。 在整个数据的传输的过程中，流动的是event，即事务保证是在event级别进行的。那么什么是event呢？Event将传输的数据进行封装，是Flume传输数据的基本单位，如果是文本文件，通常是一行记录。Event也是事务的基本单位。Event从source，流向channel，再到sink，本身为一个字节数组，并可携带headers（头信息）信息。Event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 3. Flume架构介绍 Flume之所以这么神奇，是源于它自身的一个设计，这个设计就是agent。Agent本身是一个Java进程，运行在日志收集节点——所谓日志收集节点就是服务器节点。 Agent里面包含3个核心的组件：source、channel和sink，类似生产者、仓库、消费者的架构。 Source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据,包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义。 Channel：source组件把数据收集来以后，临时存放在channel中，即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。 Sink：sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。 4. Flume的运行机制 Flume的核心就是一个agent，这个agent对外有两个进行交互的地方，一个是接受数据输入的source，一个是数据输出的sink，sink负责将数据发送到外部指定的目的地。source接收到数据之后，将数据发送给channel，chanel作为一个数据缓冲区会临时存放这些数据，随后sink会将channel中的数据发送到指定的地方，例如HDFS等。注意：只有在sink将channel中的数据成功发送出去之后，channel才会将临时数据进行删除，这种机制保证了数据传输的可靠性与安全性。 三、安装Hadoop和Flume 我的实验在HDP 2.5.0上进行，HDP安装中包含Flume，只要配置Flume服务即可。HDP的安装步骤参见“ HAWQ技术解析（二） —— 安装部署 ” 四、配置与测试 1. 建立MySQL数据库表 建立测试表并添加数据。 use test; create table wlslog (id int not null, time_stamp varchar(40), category varchar(40), type varchar(40), servername varchar(40), code varchar(40), msg varchar(40), primary key ( id ) ); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(1,&#39;apr-8-2014-7:06:16-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to standby&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(2,&#39;apr-8-2014-7:06:17-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to starting&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(3,&#39;apr-8-2014-7:06:18-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to admin&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(4,&#39;apr-8-2014-7:06:19-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to resuming&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(5,&#39;apr-8-2014-7:06:20-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000361&#39;,&#39;started weblogic adminserver&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(6,&#39;apr-8-2014-7:06:21-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to running&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(7,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); commit; 2. 建立相关目录与文件 （1）创建本地状态文件 mkdir -p /var/lib/flume cd /var/lib/flume touch sql-source.status chmod -R 777 /var/lib/flume （2）建立HDFS目标目录 hdfs dfs -mkdir -p /flume/mysql hdfs dfs -chmod -R 777 /flume/mysql 3. 准备JAR包 从 http://book2s.com/java/jar/f/flume-ng-sql-source/download-flume-ng-sql-source-1.3.7.html 下载flume-ng-sql-source-1.3.7.jar文件，并复制到Flume库目录。 cp flume-ng-sql-source-1.3.7.jar /usr/hdp/current/flume-server/lib/ 将MySQL JDBC驱动JAR包也复制到Flume库目录。 cp mysql-connector-java-5.1.17.jar /usr/hdp/current/flume-server/lib/mysql-connector-java.jar 4. 建立HAWQ外部表 create external table ext_wlslog (id int, time_stamp varchar(40), category varchar(40), type varchar(40), servername varchar(40), code varchar(40), msg varchar(40) ) location (&#39;pxf://mycluster/flume/mysql?profile=hdfstextmulti&#39;) format &#39;csv&#39; (quote=e&#39;&quot;&#39;); 5. 配置Flume 在Ambari -&gt; Flume -&gt; Configs -&gt; flume.conf中配置如下属性： agent.channels.ch1.type = memory agent.sources.sql-source.channels = ch1 agent.channels = ch1 agent.sinks = HDFS agent.sources = sql-source agent.sources.sql-source.type = org.keedio.flume.source.SQLSource agent.sources.sql-source.connection.url = jdbc:mysql://172.16.1.127:3306/test agent.sources.sql-source.user = root agent.sources.sql-source.password = 123456 agent.sources.sql-source.table = wlslog agent.sources.sql-source.columns.to.select = * agent.sources.sql-source.incremental.column.name = id agent.sources.sql-source.incremental.value = 0 agent.sources.sql-source.run.query.delay=5000 agent.sources.sql-source.status.file.path = /var/lib/flume agent.sources.sql-source.status.file.name = sql-source.status agent.sinks.HDFS.channel = ch1 agent.sinks.HDFS.type = hdfs agent.sinks.HDFS.hdfs.path = hdfs://mycluster/flume/mysql agent.sinks.HDFS.hdfs.fileType = DataStream agent.sinks.HDFS.hdfs.writeFormat = Text agent.sinks.HDFS.hdfs.rollSize = 268435456 agent.sinks.HDFS.hdfs.rollInterval = 0 agent.sinks.HDFS.hdfs.rollCount = 0 Flume在flume.conf文件中指定Source、Channel和Sink相关的配置，各属性描述如表1所示。 属性 描述 agent.channels.ch1.type Agent的channel类型 agent.sources.sql-source.channels Source对应的channel名称 agent.channels Channel名称 agent.sinks Sink名称 agent.sources Source名称 agent.sources.sql-source.type Source类型 agent.sources.sql-source.connection.url 数据库URL agent.sources.sql-source.user 数据库用户名 agent.sources.sql-source.password 数据库密码 agent.sources.sql-source.table 数据库表名 agent.sources.sql-source.columns.to.select 查询的列 agent.sources.sql-source.incremental.column.name 增量列名 agent.sources.sql-source.incremental.value 增量初始值 agent.sources.sql-source.run.query.delay 发起查询的时间间隔，单位是毫秒 agent.sources.sql-source.status.file.path 状态文件路径 agent.sources.sql-source.status.file.name 状态文件名称 agent.sinks.HDFS.channel Sink对应的channel名称 agent.sinks.HDFS.type Sink类型 agent.sinks.HDFS.hdfs.path Sink路径 agent.sinks.HDFS.hdfs.fileType 流数据的文件类型 agent.sinks.HDFS.hdfs.writeFormat 数据写入格式 agent.sinks.HDFS.hdfs.rollSize 目标文件轮转大小，单位是字节 agent.sinks.HDFS.hdfs.rollInterval hdfs sink间隔多长将临时文件滚动成最终目标文件，单位是秒；如果设置成0，则表示不根据时间来滚动文件 agent.sinks.HDFS.hdfs.rollCount 当events数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数据来滚动文件 表1 6. 运行Flume代理 保存上一步的设置，然后重启Flume服务，如图2所示。 图2 重启后，状态文件已经记录了将最新的id值7，如图3所示。 图3 查看目标路径，生成了一个临时文件，其中有7条记录，如图4所示。 图4 查询HAWQ外部表，结果也有全部7条数据，如图5所示。 图5 至此，初始数据抽取已经完成。 7. 测试准实时增量抽取 在源表中新增id为8、9、10的三条记录。 use test; insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(8,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(9,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(10,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); commit; 5秒之后查询HAWQ外部表，从图6可以看到，已经查询出全部10条数据，准实时增量抽取成功。 图6 五、方案优缺点 利用Flume采集关系数据库表数据最大的优点是配置简单，不用编程。相比tungsten-replicator的复杂性，Flume只要在flume.conf文件中配置source、channel及sink的相关属性，已经没什么难度了。而与现在很火的canal比较，虽然不够灵活，但毕竟一行代码也不用写。再有该方案采用普通SQL轮询的方式实现，具有通用性，适用于所有关系库数据源。 这种方案的缺点与其优点一样突出，主要体现在以下几方面。 在源库上执行了查询，具有入侵性。 通过轮询的方式实现增量，只能做到准实时，而且轮询间隔越短，对源库的影响越大。 只能识别新增数据，检测不到删除与更新。 要求源库必须有用于表示增量的字段。 即便有诸多局限，但用Flume抽取关系库数据的方案还是有一定的价值，特别是在要求快速部署、简化编程，又能满足需求的应用场景，对传统的Sqoop方式也不失为一种有效的补充。" />
<meta property="og:description" content="利用Flume将MySQL表数据准实时抽取到HDFS 一、为什么要用到Flume 在以前搭建HAWQ数据仓库实验环境时，我使用Sqoop抽取从MySQL数据库增量抽取数据到HDFS，然后用HAWQ的外部表进行访问。这种方式只需要很少量的配置即可完成数据抽取任务，但缺点同样明显，那就是实时性。Sqoop使用MapReduce读写数据，而MapReduce是为了批处理场景设计的，目标是大吞吐量，并不太关心低延时问题。就像实验中所做的，每天定时增量抽取数据一次。 Flume是一个海量日志采集、聚合和传输的系统，支持在日志系统中定制各类数据发送方，用于收集数据。同时，Flume提供对数据进行简单处理，并写到各种数据接受方的能力。Flume以流方式处理数据，可作为代理持续运行。当新的数据可用时，Flume能够立即获取数据并输出至目标，这样就可以在很大程度上解决实时性问题。 Flume是最初只是一个日志收集器，但随着flume-ng-sql-source插件的出现，使得Flume从关系数据库采集数据成为可能。下面简单介绍Flume，并详细说明如何配置Flume将MySQL表数据准实时抽取到HDFS。 二、Flume简介 1. Flume的概念 Flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到HDFS，简单来说flume就是收集日志的，其架构如图1所示。 图1 2. Event的概念 在这里有必要先介绍一下Flume中event的相关概念：Flume的核心是把数据从数据源（source）收集过来，在将收集到的数据送到指定的目的地（sink）。为了保证输送的过程一定成功，在送到目的地（sink）之前，会先缓存数据（channel）,待数据真正到达目的地（sink）后，Flume再删除自己缓存的数据。 在整个数据的传输的过程中，流动的是event，即事务保证是在event级别进行的。那么什么是event呢？Event将传输的数据进行封装，是Flume传输数据的基本单位，如果是文本文件，通常是一行记录。Event也是事务的基本单位。Event从source，流向channel，再到sink，本身为一个字节数组，并可携带headers（头信息）信息。Event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 3. Flume架构介绍 Flume之所以这么神奇，是源于它自身的一个设计，这个设计就是agent。Agent本身是一个Java进程，运行在日志收集节点——所谓日志收集节点就是服务器节点。 Agent里面包含3个核心的组件：source、channel和sink，类似生产者、仓库、消费者的架构。 Source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据,包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义。 Channel：source组件把数据收集来以后，临时存放在channel中，即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。 Sink：sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。 4. Flume的运行机制 Flume的核心就是一个agent，这个agent对外有两个进行交互的地方，一个是接受数据输入的source，一个是数据输出的sink，sink负责将数据发送到外部指定的目的地。source接收到数据之后，将数据发送给channel，chanel作为一个数据缓冲区会临时存放这些数据，随后sink会将channel中的数据发送到指定的地方，例如HDFS等。注意：只有在sink将channel中的数据成功发送出去之后，channel才会将临时数据进行删除，这种机制保证了数据传输的可靠性与安全性。 三、安装Hadoop和Flume 我的实验在HDP 2.5.0上进行，HDP安装中包含Flume，只要配置Flume服务即可。HDP的安装步骤参见“ HAWQ技术解析（二） —— 安装部署 ” 四、配置与测试 1. 建立MySQL数据库表 建立测试表并添加数据。 use test; create table wlslog (id int not null, time_stamp varchar(40), category varchar(40), type varchar(40), servername varchar(40), code varchar(40), msg varchar(40), primary key ( id ) ); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(1,&#39;apr-8-2014-7:06:16-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to standby&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(2,&#39;apr-8-2014-7:06:17-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to starting&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(3,&#39;apr-8-2014-7:06:18-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to admin&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(4,&#39;apr-8-2014-7:06:19-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to resuming&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(5,&#39;apr-8-2014-7:06:20-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000361&#39;,&#39;started weblogic adminserver&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(6,&#39;apr-8-2014-7:06:21-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to running&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(7,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); commit; 2. 建立相关目录与文件 （1）创建本地状态文件 mkdir -p /var/lib/flume cd /var/lib/flume touch sql-source.status chmod -R 777 /var/lib/flume （2）建立HDFS目标目录 hdfs dfs -mkdir -p /flume/mysql hdfs dfs -chmod -R 777 /flume/mysql 3. 准备JAR包 从 http://book2s.com/java/jar/f/flume-ng-sql-source/download-flume-ng-sql-source-1.3.7.html 下载flume-ng-sql-source-1.3.7.jar文件，并复制到Flume库目录。 cp flume-ng-sql-source-1.3.7.jar /usr/hdp/current/flume-server/lib/ 将MySQL JDBC驱动JAR包也复制到Flume库目录。 cp mysql-connector-java-5.1.17.jar /usr/hdp/current/flume-server/lib/mysql-connector-java.jar 4. 建立HAWQ外部表 create external table ext_wlslog (id int, time_stamp varchar(40), category varchar(40), type varchar(40), servername varchar(40), code varchar(40), msg varchar(40) ) location (&#39;pxf://mycluster/flume/mysql?profile=hdfstextmulti&#39;) format &#39;csv&#39; (quote=e&#39;&quot;&#39;); 5. 配置Flume 在Ambari -&gt; Flume -&gt; Configs -&gt; flume.conf中配置如下属性： agent.channels.ch1.type = memory agent.sources.sql-source.channels = ch1 agent.channels = ch1 agent.sinks = HDFS agent.sources = sql-source agent.sources.sql-source.type = org.keedio.flume.source.SQLSource agent.sources.sql-source.connection.url = jdbc:mysql://172.16.1.127:3306/test agent.sources.sql-source.user = root agent.sources.sql-source.password = 123456 agent.sources.sql-source.table = wlslog agent.sources.sql-source.columns.to.select = * agent.sources.sql-source.incremental.column.name = id agent.sources.sql-source.incremental.value = 0 agent.sources.sql-source.run.query.delay=5000 agent.sources.sql-source.status.file.path = /var/lib/flume agent.sources.sql-source.status.file.name = sql-source.status agent.sinks.HDFS.channel = ch1 agent.sinks.HDFS.type = hdfs agent.sinks.HDFS.hdfs.path = hdfs://mycluster/flume/mysql agent.sinks.HDFS.hdfs.fileType = DataStream agent.sinks.HDFS.hdfs.writeFormat = Text agent.sinks.HDFS.hdfs.rollSize = 268435456 agent.sinks.HDFS.hdfs.rollInterval = 0 agent.sinks.HDFS.hdfs.rollCount = 0 Flume在flume.conf文件中指定Source、Channel和Sink相关的配置，各属性描述如表1所示。 属性 描述 agent.channels.ch1.type Agent的channel类型 agent.sources.sql-source.channels Source对应的channel名称 agent.channels Channel名称 agent.sinks Sink名称 agent.sources Source名称 agent.sources.sql-source.type Source类型 agent.sources.sql-source.connection.url 数据库URL agent.sources.sql-source.user 数据库用户名 agent.sources.sql-source.password 数据库密码 agent.sources.sql-source.table 数据库表名 agent.sources.sql-source.columns.to.select 查询的列 agent.sources.sql-source.incremental.column.name 增量列名 agent.sources.sql-source.incremental.value 增量初始值 agent.sources.sql-source.run.query.delay 发起查询的时间间隔，单位是毫秒 agent.sources.sql-source.status.file.path 状态文件路径 agent.sources.sql-source.status.file.name 状态文件名称 agent.sinks.HDFS.channel Sink对应的channel名称 agent.sinks.HDFS.type Sink类型 agent.sinks.HDFS.hdfs.path Sink路径 agent.sinks.HDFS.hdfs.fileType 流数据的文件类型 agent.sinks.HDFS.hdfs.writeFormat 数据写入格式 agent.sinks.HDFS.hdfs.rollSize 目标文件轮转大小，单位是字节 agent.sinks.HDFS.hdfs.rollInterval hdfs sink间隔多长将临时文件滚动成最终目标文件，单位是秒；如果设置成0，则表示不根据时间来滚动文件 agent.sinks.HDFS.hdfs.rollCount 当events数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数据来滚动文件 表1 6. 运行Flume代理 保存上一步的设置，然后重启Flume服务，如图2所示。 图2 重启后，状态文件已经记录了将最新的id值7，如图3所示。 图3 查看目标路径，生成了一个临时文件，其中有7条记录，如图4所示。 图4 查询HAWQ外部表，结果也有全部7条数据，如图5所示。 图5 至此，初始数据抽取已经完成。 7. 测试准实时增量抽取 在源表中新增id为8、9、10的三条记录。 use test; insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(8,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(9,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(10,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); commit; 5秒之后查询HAWQ外部表，从图6可以看到，已经查询出全部10条数据，准实时增量抽取成功。 图6 五、方案优缺点 利用Flume采集关系数据库表数据最大的优点是配置简单，不用编程。相比tungsten-replicator的复杂性，Flume只要在flume.conf文件中配置source、channel及sink的相关属性，已经没什么难度了。而与现在很火的canal比较，虽然不够灵活，但毕竟一行代码也不用写。再有该方案采用普通SQL轮询的方式实现，具有通用性，适用于所有关系库数据源。 这种方案的缺点与其优点一样突出，主要体现在以下几方面。 在源库上执行了查询，具有入侵性。 通过轮询的方式实现增量，只能做到准实时，而且轮询间隔越短，对源库的影响越大。 只能识别新增数据，检测不到删除与更新。 要求源库必须有用于表示增量的字段。 即便有诸多局限，但用Flume抽取关系库数据的方案还是有一定的价值，特别是在要求快速部署、简化编程，又能满足需求的应用场景，对传统的Sqoop方式也不失为一种有效的补充。" />
<link rel="canonical" href="http://www.jfox.info/2017/%E5%88%A9%E7%94%A8flume%E5%B0%86mysql%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%87%86%E5%AE%9E%E6%97%B6%E6%8A%BD%E5%8F%96%E5%88%B0hdfs.html" />
<meta property="og:url" content="http://www.jfox.info/2017/%E5%88%A9%E7%94%A8flume%E5%B0%86mysql%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%87%86%E5%AE%9E%E6%97%B6%E6%8A%BD%E5%8F%96%E5%88%B0hdfs.html" />
<meta property="og:site_name" content="Java面试" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-01-01T15:50:50+00:00" />
<script type="application/ld+json">
{"description":"利用Flume将MySQL表数据准实时抽取到HDFS 一、为什么要用到Flume 在以前搭建HAWQ数据仓库实验环境时，我使用Sqoop抽取从MySQL数据库增量抽取数据到HDFS，然后用HAWQ的外部表进行访问。这种方式只需要很少量的配置即可完成数据抽取任务，但缺点同样明显，那就是实时性。Sqoop使用MapReduce读写数据，而MapReduce是为了批处理场景设计的，目标是大吞吐量，并不太关心低延时问题。就像实验中所做的，每天定时增量抽取数据一次。 Flume是一个海量日志采集、聚合和传输的系统，支持在日志系统中定制各类数据发送方，用于收集数据。同时，Flume提供对数据进行简单处理，并写到各种数据接受方的能力。Flume以流方式处理数据，可作为代理持续运行。当新的数据可用时，Flume能够立即获取数据并输出至目标，这样就可以在很大程度上解决实时性问题。 Flume是最初只是一个日志收集器，但随着flume-ng-sql-source插件的出现，使得Flume从关系数据库采集数据成为可能。下面简单介绍Flume，并详细说明如何配置Flume将MySQL表数据准实时抽取到HDFS。 二、Flume简介 1. Flume的概念 Flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到HDFS，简单来说flume就是收集日志的，其架构如图1所示。 图1 2. Event的概念 在这里有必要先介绍一下Flume中event的相关概念：Flume的核心是把数据从数据源（source）收集过来，在将收集到的数据送到指定的目的地（sink）。为了保证输送的过程一定成功，在送到目的地（sink）之前，会先缓存数据（channel）,待数据真正到达目的地（sink）后，Flume再删除自己缓存的数据。 在整个数据的传输的过程中，流动的是event，即事务保证是在event级别进行的。那么什么是event呢？Event将传输的数据进行封装，是Flume传输数据的基本单位，如果是文本文件，通常是一行记录。Event也是事务的基本单位。Event从source，流向channel，再到sink，本身为一个字节数组，并可携带headers（头信息）信息。Event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 3. Flume架构介绍 Flume之所以这么神奇，是源于它自身的一个设计，这个设计就是agent。Agent本身是一个Java进程，运行在日志收集节点——所谓日志收集节点就是服务器节点。 Agent里面包含3个核心的组件：source、channel和sink，类似生产者、仓库、消费者的架构。 Source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据,包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义。 Channel：source组件把数据收集来以后，临时存放在channel中，即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。 Sink：sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。 4. Flume的运行机制 Flume的核心就是一个agent，这个agent对外有两个进行交互的地方，一个是接受数据输入的source，一个是数据输出的sink，sink负责将数据发送到外部指定的目的地。source接收到数据之后，将数据发送给channel，chanel作为一个数据缓冲区会临时存放这些数据，随后sink会将channel中的数据发送到指定的地方，例如HDFS等。注意：只有在sink将channel中的数据成功发送出去之后，channel才会将临时数据进行删除，这种机制保证了数据传输的可靠性与安全性。 三、安装Hadoop和Flume 我的实验在HDP 2.5.0上进行，HDP安装中包含Flume，只要配置Flume服务即可。HDP的安装步骤参见“ HAWQ技术解析（二） —— 安装部署 ” 四、配置与测试 1. 建立MySQL数据库表 建立测试表并添加数据。 use test; create table wlslog (id int not null, time_stamp varchar(40), category varchar(40), type varchar(40), servername varchar(40), code varchar(40), msg varchar(40), primary key ( id ) ); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(1,&#39;apr-8-2014-7:06:16-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to standby&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(2,&#39;apr-8-2014-7:06:17-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to starting&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(3,&#39;apr-8-2014-7:06:18-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to admin&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(4,&#39;apr-8-2014-7:06:19-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to resuming&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(5,&#39;apr-8-2014-7:06:20-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000361&#39;,&#39;started weblogic adminserver&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(6,&#39;apr-8-2014-7:06:21-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000365&#39;,&#39;server state changed to running&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(7,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); commit; 2. 建立相关目录与文件 （1）创建本地状态文件 mkdir -p /var/lib/flume cd /var/lib/flume touch sql-source.status chmod -R 777 /var/lib/flume （2）建立HDFS目标目录 hdfs dfs -mkdir -p /flume/mysql hdfs dfs -chmod -R 777 /flume/mysql 3. 准备JAR包 从 http://book2s.com/java/jar/f/flume-ng-sql-source/download-flume-ng-sql-source-1.3.7.html 下载flume-ng-sql-source-1.3.7.jar文件，并复制到Flume库目录。 cp flume-ng-sql-source-1.3.7.jar /usr/hdp/current/flume-server/lib/ 将MySQL JDBC驱动JAR包也复制到Flume库目录。 cp mysql-connector-java-5.1.17.jar /usr/hdp/current/flume-server/lib/mysql-connector-java.jar 4. 建立HAWQ外部表 create external table ext_wlslog (id int, time_stamp varchar(40), category varchar(40), type varchar(40), servername varchar(40), code varchar(40), msg varchar(40) ) location (&#39;pxf://mycluster/flume/mysql?profile=hdfstextmulti&#39;) format &#39;csv&#39; (quote=e&#39;&quot;&#39;); 5. 配置Flume 在Ambari -&gt; Flume -&gt; Configs -&gt; flume.conf中配置如下属性： agent.channels.ch1.type = memory agent.sources.sql-source.channels = ch1 agent.channels = ch1 agent.sinks = HDFS agent.sources = sql-source agent.sources.sql-source.type = org.keedio.flume.source.SQLSource agent.sources.sql-source.connection.url = jdbc:mysql://172.16.1.127:3306/test agent.sources.sql-source.user = root agent.sources.sql-source.password = 123456 agent.sources.sql-source.table = wlslog agent.sources.sql-source.columns.to.select = * agent.sources.sql-source.incremental.column.name = id agent.sources.sql-source.incremental.value = 0 agent.sources.sql-source.run.query.delay=5000 agent.sources.sql-source.status.file.path = /var/lib/flume agent.sources.sql-source.status.file.name = sql-source.status agent.sinks.HDFS.channel = ch1 agent.sinks.HDFS.type = hdfs agent.sinks.HDFS.hdfs.path = hdfs://mycluster/flume/mysql agent.sinks.HDFS.hdfs.fileType = DataStream agent.sinks.HDFS.hdfs.writeFormat = Text agent.sinks.HDFS.hdfs.rollSize = 268435456 agent.sinks.HDFS.hdfs.rollInterval = 0 agent.sinks.HDFS.hdfs.rollCount = 0 Flume在flume.conf文件中指定Source、Channel和Sink相关的配置，各属性描述如表1所示。 属性 描述 agent.channels.ch1.type Agent的channel类型 agent.sources.sql-source.channels Source对应的channel名称 agent.channels Channel名称 agent.sinks Sink名称 agent.sources Source名称 agent.sources.sql-source.type Source类型 agent.sources.sql-source.connection.url 数据库URL agent.sources.sql-source.user 数据库用户名 agent.sources.sql-source.password 数据库密码 agent.sources.sql-source.table 数据库表名 agent.sources.sql-source.columns.to.select 查询的列 agent.sources.sql-source.incremental.column.name 增量列名 agent.sources.sql-source.incremental.value 增量初始值 agent.sources.sql-source.run.query.delay 发起查询的时间间隔，单位是毫秒 agent.sources.sql-source.status.file.path 状态文件路径 agent.sources.sql-source.status.file.name 状态文件名称 agent.sinks.HDFS.channel Sink对应的channel名称 agent.sinks.HDFS.type Sink类型 agent.sinks.HDFS.hdfs.path Sink路径 agent.sinks.HDFS.hdfs.fileType 流数据的文件类型 agent.sinks.HDFS.hdfs.writeFormat 数据写入格式 agent.sinks.HDFS.hdfs.rollSize 目标文件轮转大小，单位是字节 agent.sinks.HDFS.hdfs.rollInterval hdfs sink间隔多长将临时文件滚动成最终目标文件，单位是秒；如果设置成0，则表示不根据时间来滚动文件 agent.sinks.HDFS.hdfs.rollCount 当events数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数据来滚动文件 表1 6. 运行Flume代理 保存上一步的设置，然后重启Flume服务，如图2所示。 图2 重启后，状态文件已经记录了将最新的id值7，如图3所示。 图3 查看目标路径，生成了一个临时文件，其中有7条记录，如图4所示。 图4 查询HAWQ外部表，结果也有全部7条数据，如图5所示。 图5 至此，初始数据抽取已经完成。 7. 测试准实时增量抽取 在源表中新增id为8、9、10的三条记录。 use test; insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(8,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(9,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(10,&#39;apr-8-2014-7:06:22-pm-pdt&#39;,&#39;notice&#39;,&#39;weblogicserver&#39;,&#39;adminserver&#39;,&#39;bea-000360&#39;,&#39;server started in running mode&#39;); commit; 5秒之后查询HAWQ外部表，从图6可以看到，已经查询出全部10条数据，准实时增量抽取成功。 图6 五、方案优缺点 利用Flume采集关系数据库表数据最大的优点是配置简单，不用编程。相比tungsten-replicator的复杂性，Flume只要在flume.conf文件中配置source、channel及sink的相关属性，已经没什么难度了。而与现在很火的canal比较，虽然不够灵活，但毕竟一行代码也不用写。再有该方案采用普通SQL轮询的方式实现，具有通用性，适用于所有关系库数据源。 这种方案的缺点与其优点一样突出，主要体现在以下几方面。 在源库上执行了查询，具有入侵性。 通过轮询的方式实现增量，只能做到准实时，而且轮询间隔越短，对源库的影响越大。 只能识别新增数据，检测不到删除与更新。 要求源库必须有用于表示增量的字段。 即便有诸多局限，但用Flume抽取关系库数据的方案还是有一定的价值，特别是在要求快速部署、简化编程，又能满足需求的应用场景，对传统的Sqoop方式也不失为一种有效的补充。","@type":"BlogPosting","url":"http://www.jfox.info/2017/%E5%88%A9%E7%94%A8flume%E5%B0%86mysql%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%87%86%E5%AE%9E%E6%97%B6%E6%8A%BD%E5%8F%96%E5%88%B0hdfs.html","headline":"利用Flume将MySQL表数据准实时抽取到HDFS","dateModified":"2017-01-01T15:50:50+00:00","datePublished":"2017-01-01T15:50:50+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.jfox.info/2017/%E5%88%A9%E7%94%A8flume%E5%B0%86mysql%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%87%86%E5%AE%9E%E6%97%B6%E6%8A%BD%E5%8F%96%E5%88%B0hdfs.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://www.jfox.info/feed.xml" title="Java面试" /></head><body><header class="site-header" role="banner">
  <div class="wrapper"><a class="site-title" rel="author" href="/">Java面试</a><nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z" />
          </svg>
        </span>
      </label>

      <div class="trigger"><a class="page-link" href="/about/">About</a></div>
    </nav></div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">利用Flume将MySQL表数据准实时抽取到HDFS</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2017-01-01T15:50:50+00:00" itemprop="datePublished">Jan 1, 2017
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    
<h1 id="利用flume将mysql表数据准实时抽取到hdfs">利用Flume将MySQL表数据准实时抽取到HDFS</h1>

<h1 id="一为什么要用到flume">一、为什么要用到Flume</h1>

<p>在以前搭建HAWQ数据仓库实验环境时，我使用Sqoop抽取从MySQL数据库增量抽取数据到HDFS，然后用HAWQ的外部表进行访问。这种方式只需要很少量的配置即可完成数据抽取任务，但缺点同样明显，那就是实时性。Sqoop使用MapReduce读写数据，而MapReduce是为了批处理场景设计的，目标是大吞吐量，并不太关心低延时问题。就像实验中所做的，每天定时增量抽取数据一次。</p>

<p>Flume是一个海量日志采集、聚合和传输的系统，支持在日志系统中定制各类数据发送方，用于收集数据。同时，Flume提供对数据进行简单处理，并写到各种数据接受方的能力。Flume以流方式处理数据，可作为代理持续运行。当新的数据可用时，Flume能够立即获取数据并输出至目标，这样就可以在很大程度上解决实时性问题。</p>

<p>Flume是最初只是一个日志收集器，但随着flume-ng-sql-source插件的出现，使得Flume从关系数据库采集数据成为可能。下面简单介绍Flume，并详细说明如何配置Flume将MySQL表数据准实时抽取到HDFS。</p>

<h1 id="二flume简介">二、Flume简介</h1>

<h2 id="1-flume的概念">1. Flume的概念</h2>

<p>Flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到HDFS，简单来说flume就是收集日志的，其架构如图1所示。</p>

<p>图1</p>

<h2 id="2-event的概念">2. Event的概念</h2>

<p>在这里有必要先介绍一下Flume中event的相关概念：Flume的核心是把数据从数据源（source）收集过来，在将收集到的数据送到指定的目的地（sink）。为了保证输送的过程一定成功，在送到目的地（sink）之前，会先缓存数据（channel）,待数据真正到达目的地（sink）后，Flume再删除自己缓存的数据。</p>

<p>在整个数据的传输的过程中，流动的是event，即事务保证是在event级别进行的。那么什么是event呢？Event将传输的数据进行封装，是Flume传输数据的基本单位，如果是文本文件，通常是一行记录。Event也是事务的基本单位。Event从source，流向channel，再到sink，本身为一个字节数组，并可携带headers（头信息）信息。Event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。</p>

<h2 id="3-flume架构介绍">3. Flume架构介绍</h2>

<p>Flume之所以这么神奇，是源于它自身的一个设计，这个设计就是agent。Agent本身是一个Java进程，运行在日志收集节点——所谓日志收集节点就是服务器节点。 Agent里面包含3个核心的组件：source、channel和sink，类似生产者、仓库、消费者的架构。</p>

<ul>
  <li>Source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据,包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义。</li>
  <li>Channel：source组件把数据收集来以后，临时存放在channel中，即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。</li>
  <li>Sink：sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。</li>
</ul>

<h2 id="4-flume的运行机制">4. Flume的运行机制</h2>

<p>Flume的核心就是一个agent，这个agent对外有两个进行交互的地方，一个是接受数据输入的source，一个是数据输出的sink，sink负责将数据发送到外部指定的目的地。source接收到数据之后，将数据发送给channel，chanel作为一个数据缓冲区会临时存放这些数据，随后sink会将channel中的数据发送到指定的地方，例如HDFS等。注意：只有在sink将channel中的数据成功发送出去之后，channel才会将临时数据进行删除，这种机制保证了数据传输的可靠性与安全性。</p>

<h1 id="三安装hadoop和flume">三、安装Hadoop和Flume</h1>
<p>我的实验在HDP 2.5.0上进行，HDP安装中包含Flume，只要配置Flume服务即可。HDP的安装步骤参见“ 
 <a href="https://www.jfox.info/go.php?url=http://blog.csdn.net/wzy0623/article/details/55212318">HAWQ技术解析（二） —— 安装部署</a>
”</p>

<h1 id="四配置与测试">四、配置与测试</h1>

<h2 id="1-建立mysql数据库表">1. 建立MySQL数据库表</h2>

<p>建立测试表并添加数据。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use test;
create table  wlslog  
(id         int not null,
 time_stamp varchar(40),
 category   varchar(40),
 type       varchar(40),
 servername varchar(40),
 code       varchar(40),
 msg        varchar(40),
 primary key ( id )
);
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(1,'apr-8-2014-7:06:16-pm-pdt','notice','weblogicserver','adminserver','bea-000365','server state changed to standby');
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(2,'apr-8-2014-7:06:17-pm-pdt','notice','weblogicserver','adminserver','bea-000365','server state changed to starting');
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(3,'apr-8-2014-7:06:18-pm-pdt','notice','weblogicserver','adminserver','bea-000365','server state changed to admin');
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(4,'apr-8-2014-7:06:19-pm-pdt','notice','weblogicserver','adminserver','bea-000365','server state changed to resuming');
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(5,'apr-8-2014-7:06:20-pm-pdt','notice','weblogicserver','adminserver','bea-000361','started weblogic adminserver');
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(6,'apr-8-2014-7:06:21-pm-pdt','notice','weblogicserver','adminserver','bea-000365','server state changed to running');
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(7,'apr-8-2014-7:06:22-pm-pdt','notice','weblogicserver','adminserver','bea-000360','server started in running mode');
commit;
</code></pre></div></div>

<h2 id="2-建立相关目录与文件">2. 建立相关目录与文件</h2>

<p>（1）创建本地状态文件</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir -p /var/lib/flume
cd /var/lib/flume
touch sql-source.status
chmod -R 777 /var/lib/flume
</code></pre></div></div>

<p>（2）建立HDFS目标目录</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hdfs dfs -mkdir -p /flume/mysql
hdfs dfs -chmod -R 777 /flume/mysql
</code></pre></div></div>

<h2 id="3-准备jar包">3. 准备JAR包</h2>
<p>从 
 <a href="https://www.jfox.info/go.php?url=http://book2s.com/java/jar/f/flume-ng-sql-source/download-flume-ng-sql-source-1.3.7.html">http://book2s.com/java/jar/f/flume-ng-sql-source/download-flume-ng-sql-source-1.3.7.html</a>
下载flume-ng-sql-source-1.3.7.jar文件，并复制到Flume库目录。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cp flume-ng-sql-source-1.3.7.jar /usr/hdp/current/flume-server/lib/
</code></pre></div></div>

<p>将MySQL JDBC驱动JAR包也复制到Flume库目录。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cp mysql-connector-java-5.1.17.jar /usr/hdp/current/flume-server/lib/mysql-connector-java.jar
</code></pre></div></div>

<h2 id="4-建立hawq外部表">4. 建立HAWQ外部表</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create external table ext_wlslog
(id         int,
 time_stamp varchar(40),
 category   varchar(40),
 type       varchar(40),
 servername varchar(40),
 code       varchar(40),
 msg        varchar(40)
) location ('pxf://mycluster/flume/mysql?profile=hdfstextmulti') format 'csv' (quote=e'"'); 
</code></pre></div></div>

<h2 id="5-配置flume">5. 配置Flume</h2>

<p>在Ambari -&gt; Flume -&gt; Configs -&gt; flume.conf中配置如下属性：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>agent.channels.ch1.type = memory
agent.sources.sql-source.channels = ch1
agent.channels = ch1
agent.sinks = HDFS
agent.sources = sql-source
agent.sources.sql-source.type = org.keedio.flume.source.SQLSource
agent.sources.sql-source.connection.url = jdbc:mysql://172.16.1.127:3306/test
agent.sources.sql-source.user = root
agent.sources.sql-source.password = 123456
agent.sources.sql-source.table = wlslog
agent.sources.sql-source.columns.to.select = *
agent.sources.sql-source.incremental.column.name = id
agent.sources.sql-source.incremental.value = 0
agent.sources.sql-source.run.query.delay=5000
agent.sources.sql-source.status.file.path = /var/lib/flume
agent.sources.sql-source.status.file.name = sql-source.status
agent.sinks.HDFS.channel = ch1
agent.sinks.HDFS.type = hdfs
agent.sinks.HDFS.hdfs.path = hdfs://mycluster/flume/mysql
agent.sinks.HDFS.hdfs.fileType = DataStream
agent.sinks.HDFS.hdfs.writeFormat = Text
agent.sinks.HDFS.hdfs.rollSize = 268435456
agent.sinks.HDFS.hdfs.rollInterval = 0
agent.sinks.HDFS.hdfs.rollCount = 0
</code></pre></div></div>

<p>Flume在flume.conf文件中指定Source、Channel和Sink相关的配置，各属性描述如表1所示。</p>

<p><strong>属性</strong></p>

<p><strong>描述</strong></p>

<p>agent.channels.ch1.type</p>

<p>Agent的channel类型</p>

<p>agent.sources.sql-source.channels</p>

<p>Source对应的channel名称</p>

<p>agent.channels</p>

<p>Channel名称</p>

<p>agent.sinks</p>

<p>Sink名称</p>

<p>agent.sources</p>

<p>Source名称</p>

<p>agent.sources.sql-source.type</p>

<p>Source类型</p>

<p>agent.sources.sql-source.connection.url</p>

<p>数据库URL</p>

<p>agent.sources.sql-source.user</p>

<p>数据库用户名</p>

<p>agent.sources.sql-source.password</p>

<p>数据库密码</p>

<p>agent.sources.sql-source.table</p>

<p>数据库表名</p>

<p>agent.sources.sql-source.columns.to.select</p>

<p>查询的列</p>

<p>agent.sources.sql-source.incremental.column.name</p>

<p>增量列名</p>

<p>agent.sources.sql-source.incremental.value</p>

<p>增量初始值</p>

<p>agent.sources.sql-source.run.query.delay</p>

<p>发起查询的时间间隔，单位是毫秒</p>

<p>agent.sources.sql-source.status.file.path</p>

<p>状态文件路径</p>

<p>agent.sources.sql-source.status.file.name</p>

<p>状态文件名称</p>

<p>agent.sinks.HDFS.channel</p>

<p>Sink对应的channel名称</p>

<p>agent.sinks.HDFS.type</p>

<p>Sink类型</p>

<p>agent.sinks.HDFS.hdfs.path</p>

<p>Sink路径</p>

<p>agent.sinks.HDFS.hdfs.fileType</p>

<p>流数据的文件类型</p>

<p>agent.sinks.HDFS.hdfs.writeFormat</p>

<p>数据写入格式</p>

<p>agent.sinks.HDFS.hdfs.rollSize</p>

<p>目标文件轮转大小，单位是字节</p>

<p>agent.sinks.HDFS.hdfs.rollInterval</p>

<p>hdfs sink间隔多长将临时文件滚动成最终目标文件，单位是秒；如果设置成0，则表示不根据时间来滚动文件</p>

<p>agent.sinks.HDFS.hdfs.rollCount</p>

<p>当events数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数据来滚动文件</p>

<p>表1</p>

<h2 id="6-运行flume代理">6. 运行Flume代理</h2>

<p>保存上一步的设置，然后重启Flume服务，如图2所示。</p>

<p>图2</p>

<p>重启后，状态文件已经记录了将最新的id值7，如图3所示。</p>

<p>图3</p>

<p>查看目标路径，生成了一个临时文件，其中有7条记录，如图4所示。</p>

<p>图4</p>

<p>查询HAWQ外部表，结果也有全部7条数据，如图5所示。</p>

<p>图5</p>

<p>至此，初始数据抽取已经完成。</p>

<h2 id="7-测试准实时增量抽取">7. 测试准实时增量抽取</h2>

<p>在源表中新增id为8、9、10的三条记录。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use test;
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(8,'apr-8-2014-7:06:22-pm-pdt','notice','weblogicserver','adminserver','bea-000360','server started in running mode');
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(9,'apr-8-2014-7:06:22-pm-pdt','notice','weblogicserver','adminserver','bea-000360','server started in running mode');
insert into wlslog(id,time_stamp,category,type,servername,code,msg) values(10,'apr-8-2014-7:06:22-pm-pdt','notice','weblogicserver','adminserver','bea-000360','server started in running mode');
commit;
</code></pre></div></div>

<p>5秒之后查询HAWQ外部表，从图6可以看到，已经查询出全部10条数据，准实时增量抽取成功。</p>

<p>图6</p>

<h1 id="五方案优缺点">五、方案优缺点</h1>

<p>利用Flume采集关系数据库表数据最大的优点是配置简单，不用编程。相比tungsten-replicator的复杂性，Flume只要在flume.conf文件中配置source、channel及sink的相关属性，已经没什么难度了。而与现在很火的canal比较，虽然不够灵活，但毕竟一行代码也不用写。再有该方案采用普通SQL轮询的方式实现，具有通用性，适用于所有关系库数据源。</p>

<p>这种方案的缺点与其优点一样突出，主要体现在以下几方面。</p>

<ul>
  <li>在源库上执行了查询，具有入侵性。</li>
  <li>通过轮询的方式实现增量，只能做到准实时，而且轮询间隔越短，对源库的影响越大。</li>
  <li>只能识别新增数据，检测不到删除与更新。</li>
  <li>要求源库必须有用于表示增量的字段。</li>
</ul>

<p>即便有诸多局限，但用Flume抽取关系库数据的方案还是有一定的价值，特别是在要求快速部署、简化编程，又能满足需求的应用场景，对传统的Sqoop方式也不失为一种有效的补充。</p>

  </div><div style="width:300px;height:250px;float:left;">
    <!-- 300_250_1 -->
    <ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196"
        data-ad-slot="4142158067"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<div style="width:300px;height:250px;float:left;">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- 300-250-2 -->
    <ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196"
        data-ad-slot="5618891265"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div><a class="u-url" href="/2017/%E5%88%A9%E7%94%A8flume%E5%B0%86mysql%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%87%86%E5%AE%9E%E6%97%B6%E6%8A%BD%E5%8F%96%E5%88%B0hdfs.html" hidden></a>
</article>
<div class="PageNavigation">
  
  <a class="prev" href="/2017/%E5%88%9D%E8%AF%86eventsourcing%E5%92%8Ccqrs.html">&laquo; 初识EventSourcing和CQRS</a>
  
  
  <a class="next" href="/2017/%E5%A4%A7%E6%95%B0%E6%8D%AE-spark-%E7%AE%97%E5%AD%90-%E4%B8%80-%E6%8E%92%E5%BA%8F%E7%AE%97%E5%AD%90sortbykey%E6%9D%A5%E7%9C%8B%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E4%B8%8B%E5%A6%82%E4%BD%95.html">大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序 &raquo;</a>
  
</div>
<div class="sfix"><!-- 240x600 -->
<div class="fixedme">
    <ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9477174171188196" data-ad-slot="9597600460"
        data-ad-format="auto"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<script type="text/javascript">
    function offset(target) {
        var top = 0,
            left = 0

        while (target.offsetParent) {
            top += target.offsetTop
            left += target.offsetLeft
            target = target.offsetParent
        }

        return {
            top: top,
            left: left,
        }
    }
    window.onload = function () {
        var e = document.getElementsByClassName('sfix')[0];
        e.offset = offset(e);

        window.onscroll = function (event) {
            var e = document.getElementsByClassName('sfix')[0];
            if (window.pageYOffset && e.offset && window.pageYOffset > e.offset.top) {
                e.style.position = 'fixed';
                e.style.left = e.offset.left + 'px';
                e.style.right = 'auto';


            } else {
                e.style.position = 'absolute';
                e.style.left = 'auto';
                e.style.right = '-240px';

            }
        }
    }

</script></div>

<style>
  .wrapper {
    position: relative;
  }

  .sfix {
    position: absolute;
    top: 0;
    right: -240px;
    width: 240px;
    height: 600px;
  }

  .PageNavigation {
    font-size: 14px;
    display: block;
    width: auto;
    overflow: hidden;
    clear: both;
  }

  .PageNavigation a {
    display: block;
    width: 50%;
    float: left;
    margin: 1em 0;
  }

  .PageNavigation .next {
    text-align: right;
    float: right;
  }
</style>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Java面试</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Java面试</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
