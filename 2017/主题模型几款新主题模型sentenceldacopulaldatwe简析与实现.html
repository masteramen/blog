<!-- build time:Sat Oct 27 2018 21:00:18 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next muse use-motion"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="description" content="主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现百度最近开源了一个新的关于主题模型的项目。文档主题推断工具、语义匹配计算工具以及基于工业级语料训练的三种主题模型：LatentDirichlet Allocation(LDA)、SentenceLDA 和Topical Word Embedding(TWE)。.一、Familia简介帮Familia，打个小广告"><meta name="keywords" content="JAVA,面试"><meta property="og:type" content="article"><meta property="og:title" content="主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现"><meta property="og:url" content="http://www.jfox.info/2017/主题模型几款新主题模型sentenceldacopulaldatwe简析与实现.html"><meta property="og:site_name" content="java面试"><meta property="og:description" content="主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现百度最近开源了一个新的关于主题模型的项目。文档主题推断工具、语义匹配计算工具以及基于工业级语料训练的三种主题模型：LatentDirichlet Allocation(LDA)、SentenceLDA 和Topical Word Embedding(TWE)。.一、Familia简介帮Familia，打个小广告"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://www.jfox.info/2017/1314/4c69f6d.png"><meta property="og:image" content="http://www.jfox.info/2017/1314/60e8f5f.png"><meta property="og:image" content="http://www.jfox.info/2017/1314/70ea32b.png"><meta property="og:image" content="http://www.jfox.info/2017/1314/4eb009d.png"><meta property="og:updated_time" content="2018-10-27T12:29:39.316Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现"><meta name="twitter:description" content="主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现百度最近开源了一个新的关于主题模型的项目。文档主题推断工具、语义匹配计算工具以及基于工业级语料训练的三种主题模型：LatentDirichlet Allocation(LDA)、SentenceLDA 和Topical Word Embedding(TWE)。.一、Familia简介帮Familia，打个小广告"><meta name="twitter:image" content="http://www.jfox.info/2017/1314/4c69f6d.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.jfox.info/2017/主题模型几款新主题模型sentenceldacopulaldatwe简析与实现.html"><title>主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现 | java面试</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh_CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">java面试</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.jfox.info/2017/主题模型几款新主题模型sentenceldacopulaldatwe简析与实现.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Hello"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="java面试"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-01T23:56:54+08:00">2017-01-01</time></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现"><a href="#主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现" class="headerlink" title="主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现"></a>主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现</h1><p>百度最近开源了一个新的关于主题模型的项目。文档主题推断工具、语义匹配计算工具以及基于工业级语料训练的三种主题模型：Latent<br>Dirichlet Allocation(LDA)、SentenceLDA 和Topical Word Embedding(TWE)。<br>.</p><h1 id="一、Familia简介"><a href="#一、Familia简介" class="headerlink" title="一、Familia简介"></a>一、Familia简介</h1><p>帮Familia，打个小广告~ Familia的<a href="https://www.jfox.info/go.php?url=https://github.com/baidu/Familia">github</a><br><strong>主题模型在工业界的应用范式可以抽象为两大类: 语义表示和语义匹配。</strong></p><ul><li><p>语义表示 (Semantic Representation)<br>对文档进行主题降维，获得文档的语义表示，这些语义表示可以应用于文本分类、文本内容分析、CTR预估等下游应用。</p></li><li><p>语义匹配 (Semantic Matching)</p></li></ul><p>计算文本间的语义匹配度，我们提供两种文本类型的相似度计算方式:</p><pre><code>- 短文本-长文本相似度计算，使用场景包括文档关键词抽取、计算搜索发动机查询和网页的相似度等等。
- 长文本-长文本相似度计算，使用场景包括计算两篇文档的相似度、计算用户画像和新闻的相似度等等。
</code></pre><h3 id="Familia自带的Demo包含以下功能："><a href="#Familia自带的Demo包含以下功能：" class="headerlink" title="Familia自带的Demo包含以下功能："></a><strong>Familia自带的Demo包含以下功能：</strong></h3><p>利用主题模型对输入文档进行主题推断，以得到文档的主题降维表示。</p><ul><li>语义匹配计算</li></ul><p>计算文本之间的相似度，包括短文本-长文本、长文本-长文本间的相似度计算。</p><ul><li>模型内容展现<br>对模型的主题词，近邻词进行展现，方便用户对模型的主题有直观的理解。</li></ul><p>.</p><h1 id="二、Topical-Word-Embedding-TWE"><a href="#二、Topical-Word-Embedding-TWE" class="headerlink" title="二、Topical Word Embedding(TWE)"></a>二、Topical Word Embedding(TWE)</h1><p>Zhiyuan Liu老师的文章，<a href="https://www.jfox.info/go.php?url=http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9314/9535/">paper下载</a>以及<a href="https://www.jfox.info/go.php?url=https://github.com/largelymfs/topical_word_embeddings">github</a><br>In this way, contextual word embeddings can be flexibly obtained to measure contextual word similarity. We can also build document representations.</p><h3 id="且有三款：TWE-1，TWE-2，TWE-3，来看看和传统的skip-gram的结构区别："><a href="#且有三款：TWE-1，TWE-2，TWE-3，来看看和传统的skip-gram的结构区别：" class="headerlink" title="且有三款：TWE-1，TWE-2，TWE-3，来看看和传统的skip-gram的结构区别："></a>且有三款：TWE-1，TWE-2，TWE-3，来看看和传统的skip-gram的结构区别：</h3><p><img src="/2017/1314/4c69f6d.png" alt=""></p><h3 id="在多标签文本分类的精确度："><a href="#在多标签文本分类的精确度：" class="headerlink" title="在多标签文本分类的精确度："></a>在多标签文本分类的精确度：</h3><p><img src="/2017/1314/60e8f5f.png" alt=""></p><h3 id="百度开源项目-Familia中TWE模型的内容展现："><a href="#百度开源项目-Familia中TWE模型的内容展现：" class="headerlink" title="百度开源项目 Familia中TWE模型的内容展现："></a>百度开源项目 Familia中TWE模型的内容展现：</h3><pre><code>请输入主题编号(0-10000):    105
Embedding Result Multinomial Result ------------------------------------------------
对话                                    对话
磋商                                    合作
合作                                    中国
非方                                    磋商
探讨                                    交流
对话会议                                联合
议题                                    国家
中方                                    讨论
对话会                                  支持
交流                                    包括
</code></pre><p>第一列为基于embedding的结果，第二列为基于多项分布的结果，均按照在主题中的重要程度从大到小的顺序排序。</p><h3 id="来简单看一下train文件："><a href="#来简单看一下train文件：" class="headerlink" title="来简单看一下train文件："></a>来简单看一下train文件：</h3><pre><code>import gensim #modified gensim versionimport pre_process # read the wordmap and the tassgin file and create the sentenceimport sys
if __name__==&quot;__main__&quot;:
    if len(sys.argv)!=4:
        print&quot;Usage : python train.py wordmap tassign topic_number&quot;
        sys.exit(1) 
    reload(sys)
    sys.setdefaultencoding(&apos;utf-8&apos;)
    wordmapfile = sys.argv[1]
    tassignfile = sys.argv[2]
    topic_number = int(sys.argv[3])
    id2word = pre_process.load_id2word(wordmapfile)
    pre_process.load_sentences(tassignfile, id2word)
    sentence_word = gensim.models.word2vec.LineSentence(&quot;tmp/word.file&quot;)
    print&quot;Training the word vector...&quot;
    w = gensim.models.Word2Vec(sentence_word,size=400, workers=20)
    sentence = gensim.models.word2vec.CombinedSentence(&quot;tmp/word.file&quot;,&quot;tmp/topic.file&quot;)
    print&quot;Training the topic vector...&quot;
    w.train_topic(topic_number, sentence)
    print&quot;Saving the topic vectors...&quot;
    w.save_topic(&quot;output/topic_vector.txt&quot;)
    print&quot;Saving the word vectors...&quot;
    w.save_wordvector(&quot;output/word_vector.txt&quot;)
</code></pre><p>.</p><h1 id="三、SentenceLDA"><a href="#三、SentenceLDA" class="headerlink" title="三、SentenceLDA"></a>三、SentenceLDA</h1><p><a href="https://www.jfox.info/go.php?url=https://pdfs.semanticscholar.org/c311/778adb9484c86250e915aecd9714f4206050.pdf">paper链接</a> + <a href="https://www.jfox.info/go.php?url=https://github.com/balikasg/topicModelling/">github：balikasg/topicModelling</a></p><h3 id="SentenceLDA是什么？"><a href="#SentenceLDA是什么？" class="headerlink" title="SentenceLDA是什么？"></a><strong>SentenceLDA是什么？</strong></h3><p>an extension of LDA whose goal is to overcome this limitation by incorporating the structure of<br>the text in the generative and inference processes.</p><h3 id="SentenceLDA和LDA区别？"><a href="#SentenceLDA和LDA区别？" class="headerlink" title="SentenceLDA和LDA区别？"></a><strong>SentenceLDA和LDA区别？</strong></h3><p>LDA and senLDA differ in that the second assumes a very strong dependence of the latent topics between the words of sentences, whereas the first ssumes independence between the words of documents in general</p><h3 id="SentenceLDA和LDA两者对比实验："><a href="#SentenceLDA和LDA两者对比实验：" class="headerlink" title="SentenceLDA和LDA两者对比实验："></a><strong>SentenceLDA和LDA两者对比实验：</strong></h3><p>We illustrate the advantages of sentenceLDA by comparing it with LDA using both intrinsic (perplexity) and extrinsic (text classification) evaluation tasks on different text collections<br><img src="/2017/1314/70ea32b.png" alt=""></p><p><img src="/2017/1314/4eb009d.png" alt=""></p><h3 id="原作者的github的结果："><a href="#原作者的github的结果：" class="headerlink" title="原作者的github的结果："></a><strong>原作者的github的结果：</strong></h3><p><a href="https://www.jfox.info/go.php?url=https://github.com/balikasg/topicModelling/tree/master/senLDA">https://github.com/balikasg/topicModelling/tree/master/senLDA</a><br>截取一部分code：</p><pre><code>import numpy as np, vocabulary_sentenceLayer, string, nltk.data, sys, codecs, json, time
from nltk.tokenize import sent_tokenize
from lda_sentenceLayer import lda_gibbs_sampling1
from sklearn.cross_validation import train_test_split, StratifiedKFold
from nltk.stem import WordNetLemmatizer
from sklearn.utils import shuffle
from functions import *

path2training = sys.argv[1]
training = codecs.open(path2training, &apos;r&apos;, encoding=&apos;utf8&apos;).read().splitlines()

topics = int(sys.argv[2])
alpha, beta = 0.5 / float(topics), 0.5 / float(topics)

voca_en = vocabulary_sentenceLayer.VocabularySentenceLayer(set(nltk.corpus.stopwords.words(&apos;english&apos;)), WordNetLemmatizer(), excluds_stopwords=True)

ldaTrainingData = change_raw_2_lda_input(training, voca_en, True)
ldaTrainingData = voca_en.cut_low_freq(ldaTrainingData, 1)
iterations = 201


classificationData, y = load_classification_data(sys.argv[3], sys.argv[4])
classificationData = change_raw_2_lda_input(classificationData, voca_en, False)
classificationData = voca_en.cut_low_freq(classificationData, 1)

final_acc, final_mif, final_perpl, final_ar, final_nmi, final_p, final_r, final_f = [], [], [], [], [], [], [], []
start = time.time()
for j in range(5):
    perpl, cnt, acc, mif, ar, nmi, p, r, f = [], 0, [], [], [], [], [], [], []
    lda = lda_gibbs_sampling1(K=topics, alpha=alpha, beta=beta, docs= ldaTrainingData, V=voca_en.size())
    for i in range(iterations):
        lda.inference()
        if i % 5 == 0:
            print&quot;Iteration:&quot;, i, &quot;Perplexity:&quot;, lda.perplexity()
            features = lda.heldOutPerplexity(classificationData, 3)
            print&quot;Held-out:&quot;, features[0]
            scores = perform_class(features[1], y)
            acc.append(scores[0][0])
            mif.append(scores[1][0])
            perpl.append(features[0])
    final_acc.append(acc)
    final_mif.append(mif)
    final_perpl.append(perpl)
</code></pre><h3 id="来看看百度开源项目的最终效果，LDA和SentenceLDA的内容展现："><a href="#来看看百度开源项目的最终效果，LDA和SentenceLDA的内容展现：" class="headerlink" title="来看看百度开源项目的最终效果，LDA和SentenceLDA的内容展现："></a><strong>来看看百度开源项目的最终效果，LDA和SentenceLDA的内容展现：</strong></h3><p>LDA结果：</p><pre><code>请输入主题编号(0-1999): 105 --------------------------------------------
对话    0.189676
合作    0.0805558
中国    0.0276284
磋商    0.0269797
交流    0.021069
联合    0.0208559
国家    0.0183163
讨论    0.0154165
支持    0.0146714
包括    0.014198
</code></pre><p>第二列的数值表示词在该主题下的重要程度。<br>SentenceLDA结果：</p><pre><code>请输入主题编号(0-1999): 105 --------------------------------------------
浙江    0.0300595
浙江省  0.0290975
宁波    0.0195277
记者    0.0174735
宁波市  0.0132504
长春市  0.0123353
街道    0.0107271
吉林省  0.00954326
金华    0.00772971
公安局  0.00678163
</code></pre></div><div style="width:300px;height:250px;float:left"><ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196" data-ad-slot="4142158067"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div style="width:300px;height:250px;float:left"><ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196" data-ad-slot="5618891265"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/安装nexus并上传jar包.html" rel="next" title="安装Nexus并上传jar包"><i class="fa fa-chevron-left"></i> 安装Nexus并上传jar包</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/mysql主从复制原理探索.html" rel="prev" title="MySQL 主从复制原理探索">MySQL 主从复制原理探索 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Hello</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1358</span> <span class="site-state-item-name">posts</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现"><span class="nav-number">1.</span> <span class="nav-text">主题模型︱几款新主题模型——SentenceLDA、CopulaLDA、TWE简析与实现</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#一、Familia简介"><span class="nav-number">2.</span> <span class="nav-text">一、Familia简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Familia自带的Demo包含以下功能："><span class="nav-number">2.0.1.</span> <span class="nav-text">Familia自带的Demo包含以下功能：</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#二、Topical-Word-Embedding-TWE"><span class="nav-number">3.</span> <span class="nav-text">二、Topical Word Embedding(TWE)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#且有三款：TWE-1，TWE-2，TWE-3，来看看和传统的skip-gram的结构区别："><span class="nav-number">3.0.1.</span> <span class="nav-text">且有三款：TWE-1，TWE-2，TWE-3，来看看和传统的skip-gram的结构区别：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在多标签文本分类的精确度："><span class="nav-number">3.0.2.</span> <span class="nav-text">在多标签文本分类的精确度：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#百度开源项目-Familia中TWE模型的内容展现："><span class="nav-number">3.0.3.</span> <span class="nav-text">百度开源项目 Familia中TWE模型的内容展现：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#来简单看一下train文件："><span class="nav-number">3.0.4.</span> <span class="nav-text">来简单看一下train文件：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三、SentenceLDA"><span class="nav-number">4.</span> <span class="nav-text">三、SentenceLDA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SentenceLDA是什么？"><span class="nav-number">4.0.1.</span> <span class="nav-text">SentenceLDA是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SentenceLDA和LDA区别？"><span class="nav-number">4.0.2.</span> <span class="nav-text">SentenceLDA和LDA区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SentenceLDA和LDA两者对比实验："><span class="nav-number">4.0.3.</span> <span class="nav-text">SentenceLDA和LDA两者对比实验：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原作者的github的结果："><span class="nav-number">4.0.4.</span> <span class="nav-text">原作者的github的结果：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#来看看百度开源项目的最终效果，LDA和SentenceLDA的内容展现："><span class="nav-number">4.0.5.</span> <span class="nav-text">来看看百度开源项目的最终效果，LDA和SentenceLDA的内容展现：</span></a></li></ol></li></div></div></section></div></aside><div class="sfix"><div class="fixedme"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9477174171188196" data-ad-slot="9597600460" data-ad-format="auto"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></div></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Hello</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script></body></html><!-- rebuild by neat -->