<!DOCTYPE html>
<html lang="zh_CN"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序 | Java面试</title>
<meta name="generator" content="Jekyll v3.8.3" />
<meta property="og:title" content="大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序" />
<meta property="og:description" content="大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序" />
<link rel="canonical" href="http://www.jfox.info/2017/%E5%A4%A7%E6%95%B0%E6%8D%AE-spark-%E7%AE%97%E5%AD%90-%E4%B8%80-%E6%8E%92%E5%BA%8F%E7%AE%97%E5%AD%90sortbykey%E6%9D%A5%E7%9C%8B%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E4%B8%8B%E5%A6%82%E4%BD%95.html" />
<meta property="og:url" content="http://www.jfox.info/2017/%E5%A4%A7%E6%95%B0%E6%8D%AE-spark-%E7%AE%97%E5%AD%90-%E4%B8%80-%E6%8E%92%E5%BA%8F%E7%AE%97%E5%AD%90sortbykey%E6%9D%A5%E7%9C%8B%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E4%B8%8B%E5%A6%82%E4%BD%95.html" />
<meta property="og:site_name" content="Java面试" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-01-01T15:50:51+00:00" />
<script type="application/ld+json">
{"description":"大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序","@type":"BlogPosting","url":"http://www.jfox.info/2017/%E5%A4%A7%E6%95%B0%E6%8D%AE-spark-%E7%AE%97%E5%AD%90-%E4%B8%80-%E6%8E%92%E5%BA%8F%E7%AE%97%E5%AD%90sortbykey%E6%9D%A5%E7%9C%8B%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E4%B8%8B%E5%A6%82%E4%BD%95.html","headline":"大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序","dateModified":"2017-01-01T15:50:51+00:00","datePublished":"2017-01-01T15:50:51+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.jfox.info/2017/%E5%A4%A7%E6%95%B0%E6%8D%AE-spark-%E7%AE%97%E5%AD%90-%E4%B8%80-%E6%8E%92%E5%BA%8F%E7%AE%97%E5%AD%90sortbykey%E6%9D%A5%E7%9C%8B%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E4%B8%8B%E5%A6%82%E4%BD%95.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://www.jfox.info/feed.xml" title="Java面试" /></head><body><header class="site-header" role="banner">
  <div class="wrapper"><a class="site-title" rel="author" href="/">Java面试</a><nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z" />
          </svg>
        </span>
      </label>

      <div class="trigger"><a class="page-link" href="/about/">About</a></div>
    </nav></div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2017-01-01T15:50:51+00:00" itemprop="datePublished">Jan 1, 2017
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    
<h1 id="大数据spark-算子一排序算子sortbykey来看大数据平台下如何做排序">大数据：Spark 算子（一）排序算子sortByKey来看大数据平台下如何做排序</h1>

<p>在前面一系列博客中，特别在Shuffle博客系列中，曾描述过在生成ShuffleWrite的文件的时候，对每个partition会先进行排序并spill到文件中，最后合并成ShuffleWrite的文件，也就是每个Partition里的内容已经进行了排序，在最后的action操作的时候需要对每个executor生成的shuffle文件相同的Partition进行合并，完成Action的操作。</p>

<p><strong>排序算子和常见的reduce算子算法有何区别？</strong></p>

<p>常见的一些聚合算子，reduce算子，并不需要排序，需要做的是</p>

<ul>
  <li>将相同的hashcode分配到同一个partition，哪怕是不同的executor</li>
  <li>在做最后的合并的时候，只需要合并不同的executor里相同的partition就可以了</li>
  <li>对每个partition进行排序，考虑的是内存因数，解决的是最后的多文件使用外排序堆相同的key合并的问题</li>
</ul>

<h1 id="2-排序">2 排序</h1>

<p>先给个排序的小例子：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">package</span> <span class="n">spark</span><span class="p">.</span><span class="n">sort</span>
<span class="n">import</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">SparkConf</span>
<span class="n">import</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">SparkContext</span>
<span class="n">object</span> <span class="n">sortsample</span> <span class="p">{</span>
  <span class="n">def</span> <span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="p">:</span> <span class="k">Array</span><span class="p">[</span><span class="k">String</span><span class="p">])</span> <span class="p">{</span>
    <span class="n">val</span> <span class="n">conf</span> <span class="p">=</span> <span class="n">new</span> <span class="n">SparkConf</span><span class="p">().</span><span class="n">setAppName</span><span class="p">(</span><span class="s2">"sortsample"</span><span class="p">)</span>
    <span class="n">val</span> <span class="n">sc</span> <span class="p">=</span> <span class="n">new</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="p">)</span>
    <span class="n">var</span> <span class="n">pairs</span> <span class="p">=</span> <span class="n">sc</span><span class="p">.</span><span class="n">parallelize</span><span class="p">(</span><span class="k">Array</span><span class="p">((</span><span class="s2">"a"</span><span class="p">,</span><span class="m">0</span><span class="p">),(</span><span class="s2">"b"</span><span class="p">,</span><span class="m">0</span><span class="p">),(</span><span class="s2">"c"</span><span class="p">,</span><span class="m">3</span><span class="p">),(</span><span class="s2">"d"</span><span class="p">,</span><span class="m">6</span><span class="p">),(</span><span class="s2">"e"</span><span class="p">,</span><span class="m">0</span><span class="p">),(</span><span class="s2">"f"</span><span class="p">,</span><span class="m">0</span><span class="p">),(</span><span class="s2">"g"</span><span class="p">,</span><span class="m">3</span><span class="p">),(</span><span class="s2">"h"</span><span class="p">,</span><span class="m">6</span><span class="p">)),</span> <span class="m">2</span><span class="p">);</span>
    <span class="n">pairs</span><span class="p">.</span><span class="n">sortByKey</span><span class="p">(</span><span class="nb">true</span><span class="p">,</span> <span class="m">3</span><span class="p">).</span><span class="n">collect</span><span class="p">().</span><span class="n">foreach</span><span class="p">(</span><span class="n">println</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>核心代码： 
 OrderedRDDFunctions.scala</p>

<p>会很奇怪么？RDD里面并没有sortByKey的方法？在这里和前面博客里提到的PairRDDFunctions一样，隐式转换：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)])
    : OrderedRDDFunctions[K, V, (K, V)] = {
    new OrderedRDDFunctions[K, V, (K, V)](rdd)
  }
</code></pre></div></div>

<p>调用的是OrderedRDDFunctions.scala里的方法</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)
      : RDD[(K, V)] = self.withScope
  {
    val part = new RangePartitioner(numPartitions, self, ascending)
    new ShuffledRDD[K, V, V](self, part)
      .setKeyOrdering(if (ascending) ordering else ordering.reverse)
  }
</code></pre></div></div>

<p>对Partition采用了范围分配的策略，为何要使用范围分配的策略？这还是比较容易理解，对其它非排序类型的算子，使用散列算法，只要保证相同的key是分配在相同的partition就可以了，并不会影响相同的key的合并，计算。</p>

<p>而对排序来说，如果只是保证相同的key在相同的Partition并不足够，最后的排序还是需要合并所有的Partition进行排序合并，如果这发生在Driver端做这件事，将会非常可怕，那么我们该做些策略改变，制定一些Range，将排序上临近的分成同一个Rang，使排序相近的key分配到同一个Range上，在把Range扩大化，一个Partition管理一个Range</p>

<p><img src="7e33653.png" alt="" /></p>
<h2 id="21-分配range">2.1 分配Range</h2>

<p>range的非配不合理，会影响数据的不均衡，也就是导致executor在做同Partition排序的时候会不均衡，并行计算的整体性能往往会被单个最糟糕的运行节点所拖累，如果提高运算的速度，需要考虑数据分配的均衡性。</p>

<h3 id="211-每个区块采样大小">2.1.1 每个区块采样大小</h3>

<p>获取所有的key，然后在来制定区间，这显然是不明智的，这样会变成一个全量数据的排序，然后在重新划分数据，所需需要采样部分数据进行区间划分</p>

<p>Partitioner.scala rangeBounds</p>

<p>代码如下：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val sampleSize = math.min(20.0 * partitions, 1e6)
      // Assume the input partitions are roughly balanced and over-sample a little bit.
      val sampleSizePerPartition = math.ceil(3.0 * sampleSize / rdd.partitions.length).toInt
      val (numItems, sketched) = RangePartitioner.sketch(rdd.map(_._1), sampleSizePerPartition)
</code></pre></div></div>

<ul>
  <li>
    <p>partitions: 参数在指定sortByKey的时候设置的区块大小：3</p>

    <p>pairs.sortByKey(true, 3)</p>
  </li>
  <li>
    <p>rdd.partitions: 指的是在数据的分区块大小:2</p>

    <p>sc.parallelize(Array((“a”,0),(“b”,0),(“c”,3),(“d”,6),(“e”,0),(“f”,0),(“g”,3),(“h”,6)), 2)</p>
  </li>
</ul>

<p>需要采样的数量是通过几个固定参数来设计的</p>

<h3 id="212-sketch采样蓄水池采样法">2.1.2 Sketch采样(蓄水池采样法)</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def sketch[K : ClassTag](
      rdd: RDD[K],
      sampleSizePerPartition: Int): (Long, Array[(Int, Long, Array[K])]) = {
    val shift = rdd.id
    // val classTagK = classTag[K] // to avoid serializing the entire partitioner object
    val sketched = rdd.mapPartitionsWithIndex { (idx, iter) =&gt;
      val seed = byteswap32(idx ^ (shift &lt;&lt; 16))
      val (sample, n) = SamplingUtils.reservoirSampleAndCount(
        iter, sampleSizePerPartition, seed)
      Iterator((idx, n, sample))
    }.collect()
    val numItems = sketched.map(_._2).sum
    (numItems, sketched)
  }
</code></pre></div></div>

<p>mapPartitionsWithIndex, collection 都是需要在提交job进行运算的，也就是采样的过程中，是通过executor执行了一次job</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def reservoirSampleAndCount[T: ClassTag](
      input: Iterator[T],
      k: Int,
      seed: Long = Random.nextLong())
    : (Array[T], Long) = {
    val reservoir = new Array[T](k)
    // Put the first k elements in the reservoir.
    var i = 0
    while (i &lt; k &amp;&amp; input.hasNext) {
      val item = input.next()
      reservoir(i) = item
      i += 1
    }
    // If we have consumed all the elements, return them. Otherwise do the replacement.
    if (i &lt; k) {
      // If input size &lt; k, trim the array to return only an array of input size.
      val trimReservoir = new Array[T](i)
      System.arraycopy(reservoir, 0, trimReservoir, 0, i)
      (trimReservoir, i)
    } else {
      // If input size &gt; k, continue the sampling process.
      var l = i.toLong
      val rand = new XORShiftRandom(seed)
      while (input.hasNext) {
        val item = input.next()
        l += 1
        // There are k elements in the reservoir, and the l-th element has been
        // consumed. It should be chosen with probability k/l. The expression
        // below is a random long chosen uniformly from [0,l)
        val replacementIndex = (rand.nextDouble() * l).toLong
        if (replacementIndex &lt; k) {
          reservoir(replacementIndex.toInt) = item
        }
      }
      (reservoir, l)
    }
  }
</code></pre></div></div>

<p>函数 
 reservoirSampleAndCount采样</p>

<ul>
  <li>当数据小于要采样的集合的时候，可以使用数据为样本</li>
  <li>
    <p>当数据集合超过需要采样数目的时候会继续遍历整个数据集合，通过随机数进行位置的随机替换，保证采样数据的随机性</p>

    <p>返回的结果里包含了总数据集，区块编号，区块的数量，每个区块的采样集</p>
  </li>
</ul>

<h3 id="213-重新采样">2.1.3 重新采样</h3>

<p>为了避免某些区块的数据量过大，设置了一个阈值：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val fraction = math.min(sampleSize / math.max(numItems, 1L), 1.0)
</code></pre></div></div>

<p>阈值＝采样数除于总数据量，当某个区块的数据量＊阈值大于每个区的采样率的时候，认为这个区块的采样率是不足的，需要重新采样</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val imbalanced = new PartitionPruningRDD(rdd.map(_._1), imbalancedPartitions.contains)
          val seed = byteswap32(-rdd.id - 1)
          val reSampled = imbalanced.sample(withReplacement = false, fraction, seed).collect()
          val weight = (1.0 / fraction).toFloat
          candidates ++= reSampled.map(x =&gt; (x, weight))
</code></pre></div></div>

<h3 id="214-采样集key的权重">2.1.4 采样集key的权重</h3>

<p>上面我们对每个区进行了相同数量的采样，但是每个区的数量有可能是不均衡的，需要对每个区采样的key进行权重设置，尽量分配高权重给数据量多的区</p>

<p>权重因子：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val weight = (n.toDouble / sample.length).toFloat
</code></pre></div></div>

<p>n 是区的数据数量</p>

<p>sample 是采样的数量</p>

<p>这里权重的最小值是1，因为采样的数量肯定是小于等于数据</p>

<p>当数据量大于采样数量的时候，每个区的采样数量是相同的，那么意味着区的数据量越大，该区块的key的权重也就越大</p>

<h3 id="215-分配每个区块的range">2.1.5 分配每个区块的range</h3>

<p>样本已经采集好了，现在需要对依据样本进行区块的range进行分配</p>

<ul>
  <li>先对样本进行排序</li>
  <li>依据每个样本的权重计算每个区块平均所分配的权重</li>
  <li>
    <p>最后通过每个区分配的权重<strong>按照顺序</strong>来决定获取哪些样本用作range，一个区分配一个样本区间</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def determineBounds[K : Ordering : ClassTag](
    candidates: ArrayBuffer[(K, Float)],
    partitions: Int): Array[K] = {
  val ordering = implicitly[Ordering[K]]
  val ordered = candidates.sortBy(_._1)
  val numCandidates = ordered.size
  val sumWeights = ordered.map(_._2.toDouble).sum
  val step = sumWeights / partitions
  var cumWeight = 0.0
  var target = step
  val bounds = ArrayBuffer.empty[K]
  var i = 0
  var j = 0
  var previousBound = Option.empty[K]
  while ((i &lt; numCandidates) &amp;&amp; (j &lt; partitions - 1)) {
    val (key, weight) = ordered(i)
    cumWeight += weight
    if (cumWeight &gt;= target) {
      // Skip duplicate values.
      if (previousBound.isEmpty || ordering.gt(key, previousBound.get)) {
        bounds += key
        target += step
        j += 1
        previousBound = Some(key)
      }
    }
    i += 1
  }
  bounds.toArray
}
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="22-shufflewriter">2.2 ShuffleWriter</h2>

<p>在前面的一序列的博客里都是介绍了SortShuffleWrite，在sortByKey的情况下使用了BypassMergeSortShuffleWriter，把焦点聚焦到key如何分配到Partitioner和每个Partition的文件将会如何写入key，value</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>while (records.hasNext()) {
      final Product2&lt;K, V&gt; record = records.next();
      final K key = record._1();
      partitionWriters[partitioner.getPartition(key)].write(key, record._2());
    }
</code></pre></div></div>

<h3 id="221-分配key到partition">2.2.1 分配key到Partition</h3>

<p>在函数调用了，partitioner.getPartition方法，还是回到RangePartitioner类中</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> def getPartition(key: Any): Int = {
    val k = key.asInstanceOf[K]
    var partition = 0
    if (rangeBounds.length &lt;= 128) {
      // If we have less than 128 partitions naive search
      while (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) {
        partition += 1
      }
    } else {
      // Determine which binary search method to use only once.
      partition = binarySearch(rangeBounds, k)
      // binarySearch either returns the match location or -[insertion point]-1
      if (partition &lt; 0) {
        partition = -partition-1
      }
      if (partition &gt; rangeBounds.length) {
        partition = rangeBounds.length
      }
    }
    if (ascending) {
      partition
    } else {
      rangeBounds.length - partition
    }
  }
</code></pre></div></div>

<ul>
  <li>当Partition的分配数小于128的时候，轮训的查找每个Partition</li>
  <li>当Partition大于128的时候，使用二分法查找Partition</li>
</ul>

<h3 id="222-生成shuffle文件">2.2.2 生成shuffle文件</h3>

<ul>
  <li>基于前面对key进行排序的partition的分配，写到对应的partition文件中</li>
  <li>合并Partition文件生成index和data文件（shuffle_shuffleid_mapid_0.index）（shuffle_shuffleid_mapid_0.data）因为Partition已经合并了，最后一位reduceID都是为0</li>
</ul>

<p><strong>注意：在这里并没有象SortShuffleWrite 对每个Partition进行排序,Spill 文件，最后合并文件，而是直接写到了Partition文件中。</strong></p>
<h2 id="23-shuffle-read">2.3 Shuffle Read</h2>

<p>在 
  BlockStoreShuffleReader的read函数里</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  dep.keyOrdering match {
      case Some(keyOrd: Ordering[K]) =&gt;
        // Create an ExternalSorter to sort the data. Note that if spark.shuffle.spill is disabled,
        // the ExternalSorter won't spill to disk.
        val sorter =
          new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)
        sorter.insertAll(aggregatedIter)
        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)
        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)
        context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)
        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())
      case None =&gt;
        aggregatedIter
    }

  
  
ExternalSorter.insertAll函数 
   

 while (records.hasNext) {
        addElementsRead()
        val kv = records.next()
        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])
        maybeSpillCollection(usingMap = false)
      }
</code></pre></div></div>

<p>ExternalSorter函数，这个函数在前面的 
  <a href="https://www.jfox.info/go.php?url=http://blog.csdn.net/raintungli/article/details/70807376">这篇博客</a>里介绍的比较清楚，但这里的结构体和博客介绍的不太一样</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> @volatile private var map = new PartitionedAppendOnlyMap[K, C]
  @volatile private var buffer = new PartitionedPairBuffer[K, C]
</code></pre></div></div>

<p>在reduceByKey的这些算子，相同的Key是需要合并的，需要使用Map结构处理相同的Key的值的合并问题，但对排序来说，是不需要相同的值合并，使用Array结构就足够了。</p>

<p>在Spark上实现Map、Array都使用了数组的结构，并没有用链表结构。</p>

<p>在上图的PartitionPairBuffer结构中，有几个点要注意</p>

<ol>
  <li>插入KV结构的时候，并没有进行排序，也就是在处理相同的Partition的时候合并Key值并没有进行</li>
  <li>依然会存在当内存不够，Spill到磁盘的情况，关于Spill请具体参考<a href="https://www.jfox.info/go.php?url=http://blog.csdn.net/raintungli/article/details/70807376#t3">博客</a>链接</li>
</ol>

<h3 id="231-排序">2.3.1 排序</h3>

<p>当ExternalSorter.insertAll函数完成后，才会构建一个排序的迭代器</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])] = {
         
   
   
-  val collection: WritablePartitionedPairCollection[K, C] = if (usingMap) map else buffer

    val usingMap = aggregator.isDefined
    if (spills.isEmpty) {
      // Special case: if we have only in-memory data, we don't need to merge streams, and perhaps
      // we don't even need to sort by anything other than partition ID
      if (!ordering.isDefined) {
        // The user hasn't requested sorted keys, so only sort by partition ID, not key
        groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(None)))
      } else {
        // We do need to sort by both partition ID and key
        groupByPartition(destructiveIterator(
          collection.partitionedDestructiveSortedIterator(Some(keyComparator))))
      }
    } else {
      // Merge spilled and in-memory data
      merge(spills, destructiveIterator(
        collection.partitionedDestructiveSortedIterator(comparator)))
    }
  }
</code></pre></div></div>

<p>这里分成两种情况：</p>

<ul>
  <li>
    <p>还在内存里没有Spill到文件中去，这时候构建一个内存里的PartitionedDestructiveSortedIterator迭代器，在迭代器中已经排序好了PartitionPairBuffer里的内容</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/** Iterate through the data in a given order. For this class this is not really destructive. */
override def partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])
  : Iterator[((Int, K), V)] = {
  val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)
  new Sorter(new KVArraySortDataFormat[(Int, K), AnyRef]).sort(data, 0, curSize, comparator)
  iterator
}
</code></pre></div>    </div>
  </li>
  <li>
    <p>Spill到文件里的，文件里的已经排好序了，还需要对内存里的PartitionPairBuffer进行排序（和前面相同的处理），然后对文件和内存进行外排序（外排序可参考<a href="https://www.jfox.info/go.php?url=http://blog.csdn.net/raintungli/article/details/70807376#t5">博客</a>）</p>
  </li>
</ul>

<h2 id="24-最后的归并">2.4 最后的归并</h2>
<p>在Dag-scheduler-event-loop 线程中会处理每个executor返回的结果，也就是刚才的Partition的结果</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  private[scheduler] def handleTaskCompletion(event: CompletionEvent) {
....
  case Success =&gt;
        stage.pendingPartitions -= task.partitionId
        task match {
          case rt: ResultTask[_, _] =&gt;
            // Cast to ResultStage here because it's part of the ResultTask
            // TODO Refactor this out to a function that accepts a ResultStage
            val resultStage = stage.asInstanceOf[ResultStage]
            resultStage.activeJob match {
              case Some(job) =&gt;
                if (!job.finished(rt.outputId)) {
                  updateAccumulators(event)
                  job.finished(rt.outputId) = true
                  job.numFinished += 1
                  // If the whole job has finished, remove it
                  if (job.numFinished == job.numPartitions) {
                    markStageAsFinished(resultStage)
                    cleanupStateForJobAndIndependentStages(job)
                    listenerBus.post(
                      SparkListenerJobEnd(job.jobId, clock.getTimeMillis(), JobSucceeded))
                  }
                  // taskSucceeded runs some user code that might throw an exception. Make sure
                  // we are resilient against that.
                  try {
                    job.listener.taskSucceeded(rt.outputId, event.result)
                  } catch {
                    case e: Exception =&gt;
                      // TODO: Perhaps we want to mark the resultStage as failed?
                      job.listener.jobFailed(new SparkDriverExecutionException(e))
                  }
                }
}
</code></pre></div></div>

<p>通过方法taskSucceeded的方法进行每个Partition的合并</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>job.listener.taskSucceeded(rt.outputId, event.result)

  override def taskSucceeded(index: Int, result: Any): Unit = {
    // resultHandler call must be synchronized in case resultHandler itself is not thread safe.
    synchronized {
      resultHandler(index, result.asInstanceOf[T])
    }
    if (finishedTasks.incrementAndGet() == totalTasks) {
      jobPromise.success(())
    }
  }
</code></pre></div></div>

<p>调用了resultHandler方法，继续看看resultHandler是怎样定义的</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int]): Array[U] = {
    val results = new Array[U](partitions.size)
    runJob[T, U](rdd, func, partitions, (index, res) =&gt; results(index) = res)
    results
  }
</code></pre></div></div>

<p>在runJob的方法里</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int],
      resultHandler: (Int, U) =&gt; Unit): Unit = {
    if (stopped.get()) {
      throw new IllegalStateException("SparkContext has been shutdown")
    }
    val callSite = getCallSite
    val cleanedFunc = clean(func)
    logInfo("Starting job: " + callSite.shortForm)
    if (conf.getBoolean("spark.logLineage", false)) {
      logInfo("RDD's recursive dependencies:n" + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()
  }
</code></pre></div></div>

<p>也就是：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(index, res) =&gt; results(index) = res)
</code></pre></div></div>

<p>构建了一个数组result，将每个Partition的数值保存到result的数组里</p>

<p>result[0]=partition[0] =array(tuple,tuple…..)</p>

<p><strong>什么时候对所有的Partition最后合并呢？</strong></p>

<p>来看RDD的collect算子</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def collect(): Array[T] = withScope {
    val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)
    Array.concat(results: _*)
  }
</code></pre></div></div>

<p>runJob返回的是result的数组，每个Partition是管理不同的范围，最后的合并只要简单的将不同的Partition合并就可以了</p>

<h1 id="3-排序完整的流程">3. 排序完整的流程</h1>

<ul>
  <li>Driver 提交一个采样任务，需要Executor对每个Partition进行数据采样，数据采样是一次全数据的扫描</li>
  <li>Driver 获取采样数据，每个Partition的数据量，依据数据量的权重，进行Range的分配</li>
  <li>Driver 开始进行排序，先提交ShuffleMapTask ，Executor对分配到自己的数据基于Range进行Partition的分配，直接写入Shuffle文件中</li>
  <li>Driver 提交ResultTask，Executor读取Shuffle文件中相同的Partition进行合并（相同的key不做值的合并）、排序</li>
  <li>Driver 接收到ResultTask的值后，最后进行不同的Partition数据合并</li>
</ul>

  </div>
  <div style="width:300px;height:250px;float:left;">
    <!-- 300_250_1 -->
    <ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196"
        data-ad-slot="4142158067"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<div style="width:300px;height:250px;float:left;">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- 300-250-2 -->
    <ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196"
        data-ad-slot="5618891265"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div><a class="u-url" href="/2017/%E5%A4%A7%E6%95%B0%E6%8D%AE-spark-%E7%AE%97%E5%AD%90-%E4%B8%80-%E6%8E%92%E5%BA%8F%E7%AE%97%E5%AD%90sortbykey%E6%9D%A5%E7%9C%8B%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E4%B8%8B%E5%A6%82%E4%BD%95.html" hidden></a>
</article>
<div class="PageNavigation">
  
  <a class="prev" href="/2017/%E5%88%A9%E7%94%A8flume%E5%B0%86mysql%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%87%86%E5%AE%9E%E6%97%B6%E6%8A%BD%E5%8F%96%E5%88%B0hdfs.html">&laquo; 利用Flume将MySQL表数据准实时抽取到HDFS</a>
  
  
  <a class="next" href="/2017/springboot-thymeleaf%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%E7%99%BB%E9%99%86%E6%B3%A8%E5%86%8C%E5%8F%8A%E8%AE%B0%E8%B4%A6%E5%8A%9F%E8%83%BD.html">SpringBoot+thymeleaf简单实现登陆注册及记账功能 &raquo;</a>
  
</div>
<div class="sfix"><!-- 240x600 -->
<div class="fixedme">
    <ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9477174171188196" data-ad-slot="9597600460"
        data-ad-format="auto"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<script type="text/javascript">
    function offset(target) {
        var top = 0,
            left = 0

        while (target.offsetParent) {
            top += target.offsetTop
            left += target.offsetLeft
            target = target.offsetParent
        }

        return {
            top: top,
            left: left,
        }
    }
    window.onload = function () {
        var e = document.getElementsByClassName('sfix')[0];
        e.offset = offset(e);

        window.onscroll = function (event) {
            var e = document.getElementsByClassName('sfix')[0];
            if (window.pageYOffset && e.offset && window.pageYOffset > e.offset.top) {
                e.style.position = 'fixed';
                e.style.left = e.offset.left + 'px';
                e.style.right = 'auto';


            } else {
                e.style.position = 'absolute';
                e.style.left = 'auto';
                e.style.right = '-240px';

            }
        }
    }

</script></div>

<style>
  .wrapper {
    position: relative;
  }

  .sfix {
    position: absolute;
    top: 0;
    right: -240px;
    width: 240px;
    height: 600px;
  }

  .PageNavigation {
    font-size: 14px;
    display: block;
    width: auto;
    overflow: hidden;
    clear: both;
  }

  .PageNavigation a {
    display: block;
    width: 50%;
    float: left;
    margin: 1em 0;
  }

  .PageNavigation .next {
    text-align: right;
    float: right;
  }
</style>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Java面试</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Java面试</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
