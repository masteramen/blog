<!DOCTYPE html>
<html lang="zh_CN"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>[原创] 执行ELL的demo程序cntkDemo.py时程序僵死的问题 | Java面试</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="[原创] 执行ELL的demo程序cntkDemo.py时程序僵死的问题" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="OS：Ubuntu 14.04 在台式机上执行ELL的demo程序 cntkDemo.py 时，可能会遇到程序僵死的问题。 cntkDemo.py 这个程序会调用OpenCV，在一个GUI窗口中显示USB摄像头拍摄的实时视频流，而僵死的现象正是：执行到弹出GUI窗口显示摄像头拍摄的视频流的代码的时候，程序进入僵死状态，不能执行后续逻辑。此时，只能Ctrl+C终止掉程序。 我的Ubuntu 14.04是一台老爷机，性能非常差，我觉得这有可能程序僵死的原因之一？我试了几次都是这样，于是我打算换一个思路来跑这个demo，不再纠结于解决窗口僵死的问题。 文章来源： https://www.codelast.com/ 先来看一下原版的 cntkDemo.py 部分代码： while (True): # Grab next frame ret, frame = cap.read() # Prepare the image to send to the model. # This involves scaling to the required input dimension and re-ordering from BGR to RGB data = helper.prepare_image_for_predictor(frame) # Get the model to classify the image, by returning a list of probabilities for the classes it can detect predictions = model.Predict(data) # Get the (at most) top 5 predictions that meet our threshold. This is returned as a list of tuples, # each with the text label and the prediction score. top5 = helper.get_top_n(predictions, 5) # Turn the top5 into a text string to display text = &quot;&quot;.join([str(element[0]) + &quot;(&quot; + str(int(100*element[1])) + &quot;%) &quot; for element in top5]) # Draw the text on the frame frameToShow = frame helper.draw_label(frameToShow, text) helper.draw_fps(frameToShow) # Show the new frame cv2.imshow(&#39;frame&#39;, frameToShow) # Wait for Esc key if cv2.waitKey(1) &amp; 0xFF == 27: break 这段代码的注释非常清晰，它的功能是：在一个无限循环中，不断地去抓取USB摄像头拍摄的一帧图像，然后用model预测其分类及概率，最后再把预测结果叠加显示在GUI窗口中，类似于下面这样： 既然 cntkDemo.py 主要是为了测试model能不能正常跑，那么我在命令行以文字形式显示预测结果也是一样的啊，没有必要非得在GUI窗口中展示。 文章来源： https://www.codelast.com/ 于是我把程序改成了下面这样（完整程序）： import sys import os import numpy as np import cv2 import time import findEll import cntk_to_ell import modelHelper as mh def get_ell_predictor(modelConfig): &quot;&quot;&quot;Imports a model and returns an ELL.Predictor.&quot;&quot;&quot; return cntk_to_ell.predictor_from_cntk_model(modelConfig.model_files[0]) def main(): if (not os.path.exists(&#39;VGG16_ImageNet_Caffe.model&#39;)): print(&quot;Please download the &#39;VGG16_ImageNet_Caffe.model&#39; file, see README.md&quot;) sys.exit(1) # ModelConfig for VGG16 model from CNTK Model Gallery # Follow the instructions in README.md to download the model if you intend to use it. helper = mh.ModelHelper(&quot;VGG16ImageNet&quot;, [&quot;VGG16_ImageNet_Caffe.model&quot;], &quot;cntkVgg16ImageNetLabels.txt&quot;, scaleFactor=1.0) # Import the model model = get_ell_predictor(helper) # Save the model helper.save_ell_predictor_to_file(model, &quot;vgg16ImageNet.map&quot;) camera = 0 if (len(sys.argv) &gt; 1): camera = int(sys.argv[1]) # Start video capture device cap = cv2.VideoCapture(camera) while (True): print(&#39;Read a frame from camera...&#39;) ret, frame = cap.read() # Prepare the image to send to the model. # This involves scaling to the required input dimension and re-ordering from BGR to RGB data = helper.prepare_image_for_predictor(frame) # Get the model to classify the image, by returning a list of probabilities for the classes it can detect predictions = model.Predict(data) # Get the (at most) top 5 predictions that meet our threshold. This is returned as a list of tuples, # each with the text label and the prediction score. top5 = helper.get_top_n(predictions, 5) # Turn the top5 into a text string to display text = &quot;&quot;.join([str(element[0]) + &quot;(&quot; + str(int(100*element[1])) + &quot;%) &quot; for element in top5]) # Output the text on command line print(text) if __name__ == &quot;__main__&quot;: main() OpenBLAS : Your OS does not support AVX instructions. OpenBLAS is using Nehalem kernels as a fallback, which may give poorer performance. Read a frame from camera, time 1 Frame 1 saved to disk Read a frame from camera, time 2 Frame 2 saved to disk Read a frame from camera, time 3 Frame 3 saved to disk Read a frame from camera, time 4 Frame 4 saved to disk Read a frame from camera, time 5 Frame 5 saved to disk Loading… Selected CPU as the process wide default device. Finished loading. Pre-processing… Will not process Dropout – skipping this layer as irrelevant. Will not process Dropout – skipping this layer as irrelevant. Will not process Combine – skipping this layer as irrelevant. Convolution : 226x226x3 -&gt; 224x224x64 padding 1 ReLU : 224x224x64 -&gt; 226x226x64 padding 0 Convolution : 226x226x64 -&gt; 224x224x64 padding 1 ReLU : 224x224x64 -&gt; 224x224x64 padding 0 Pooling : 224x224x64 -&gt; 114x114x64 padding 0 Convolution : 114x114x64 -&gt; 112x112x128 padding 1 ReLU : 112x112x128 -&gt; 114x114x128 padding 0 Convolution : 114x114x128 -&gt; 112x112x128 padding 1 ReLU : 112x112x128 -&gt; 112x112x128 padding 0 Pooling : 112x112x128 -&gt; 58x58x128 padding 0 Convolution : 58x58x128 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 58x58x256 padding 0 Convolution : 58x58x256 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 58x58x256 padding 0 Convolution : 58x58x256 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 56x56x256 padding 0 Pooling : 56x56x256 -&gt; 30x30x256 padding 0 Convolution : 30x30x256 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 30x30x512 padding 0 Convolution : 30x30x512 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 30x30x512 padding 0 Convolution : 30x30x512 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 28x28x512 padding 0 Pooling : 28x28x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 14x14x512 padding 0 Pooling : 14x14x512 -&gt; 7x7x512 padding 0 linear : 7x7x512 -&gt; 1x1x4096 padding 0 ReLU : 1x1x4096 -&gt; 1x1x4096 padding 0 linear : 1x1x4096 -&gt; 1x1x4096 padding 0 ReLU : 1x1x4096 -&gt; 1x1x4096 padding 0 linear : 1x1x4096 -&gt; 1x1x1000 padding 0 Softmax : 1x1x1000 -&gt; 1x1x1000 padding 0 Finished pre-processing. Constructing equivalent ELL layers from CNTK… Converting layer conv1_1: Convolution(data: Tensor[3,224,224]) -&gt; Tensor[64,224,224] Converting layer relu1_1: ReLU(conv1_1: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer conv1_2: Convolution(relu1_1: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer relu1_2: ReLU(conv1_2: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer pool1: Pooling(relu1_2: Tensor[64,224,224]) -&gt; Tensor[64,112,112] Converting layer conv2_1: Convolution(pool1: Tensor[64,112,112]) -&gt; Tensor[128,112,112] Converting layer relu2_1: ReLU(conv2_1: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer conv2_2: Convolution(relu2_1: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer relu2_2: ReLU(conv2_2: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer pool2: Pooling(relu2_2: Tensor[128,112,112]) -&gt; Tensor[128,56,56] Converting layer conv3_1: Convolution(pool2: Tensor[128,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_1: ReLU(conv3_1: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer conv3_2: Convolution(relu3_1: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_2: ReLU(conv3_2: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer conv3_3: Convolution(relu3_2: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_3: ReLU(conv3_3: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer pool3: Pooling(relu3_3: Tensor[256,56,56]) -&gt; Tensor[256,28,28] Converting layer conv4_1: Convolution(pool3: Tensor[256,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_1: ReLU(conv4_1: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer conv4_2: Convolution(relu4_1: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_2: ReLU(conv4_2: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer conv4_3: Convolution(relu4_2: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_3: ReLU(conv4_3: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer pool4: Pooling(relu4_3: Tensor[512,28,28]) -&gt; Tensor[512,14,14] Converting layer conv5_1: Convolution(pool4: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_1: ReLU(conv5_1: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer conv5_2: Convolution(relu5_1: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_2: ReLU(conv5_2: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer conv5_3: Convolution(relu5_2: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_3: ReLU(conv5_3: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer pool5: Pooling(relu5_3: Tensor[512,14,14]) -&gt; Tensor[512,7,7] Converting layer fc6: linear(pool5: Tensor[512,7,7]) -&gt; Tensor[4096] Converting layer relu6: ReLU(fc6: Tensor[4096]) -&gt; Tensor[4096] Converting layer fc7: linear(drop6: Tensor[4096]) -&gt; Tensor[4096] Converting layer relu7: ReLU(fc7: Tensor[4096]) -&gt; Tensor[4096] Converting layer fc8: linear(drop7: Tensor[4096]) -&gt; Tensor[1000] Converting layer prob: Softmax(fc8: Tensor[1000]) -&gt; Tensor[1000] …Finished constructing ELL layers. lighter, light, igniter, ignitor(28%) lighter, light, igniter, ignitor(28%) lighter, light, igniter, ignitor(32%) lighter, light, igniter, ignitor(30%) ……" />
<meta property="og:description" content="OS：Ubuntu 14.04 在台式机上执行ELL的demo程序 cntkDemo.py 时，可能会遇到程序僵死的问题。 cntkDemo.py 这个程序会调用OpenCV，在一个GUI窗口中显示USB摄像头拍摄的实时视频流，而僵死的现象正是：执行到弹出GUI窗口显示摄像头拍摄的视频流的代码的时候，程序进入僵死状态，不能执行后续逻辑。此时，只能Ctrl+C终止掉程序。 我的Ubuntu 14.04是一台老爷机，性能非常差，我觉得这有可能程序僵死的原因之一？我试了几次都是这样，于是我打算换一个思路来跑这个demo，不再纠结于解决窗口僵死的问题。 文章来源： https://www.codelast.com/ 先来看一下原版的 cntkDemo.py 部分代码： while (True): # Grab next frame ret, frame = cap.read() # Prepare the image to send to the model. # This involves scaling to the required input dimension and re-ordering from BGR to RGB data = helper.prepare_image_for_predictor(frame) # Get the model to classify the image, by returning a list of probabilities for the classes it can detect predictions = model.Predict(data) # Get the (at most) top 5 predictions that meet our threshold. This is returned as a list of tuples, # each with the text label and the prediction score. top5 = helper.get_top_n(predictions, 5) # Turn the top5 into a text string to display text = &quot;&quot;.join([str(element[0]) + &quot;(&quot; + str(int(100*element[1])) + &quot;%) &quot; for element in top5]) # Draw the text on the frame frameToShow = frame helper.draw_label(frameToShow, text) helper.draw_fps(frameToShow) # Show the new frame cv2.imshow(&#39;frame&#39;, frameToShow) # Wait for Esc key if cv2.waitKey(1) &amp; 0xFF == 27: break 这段代码的注释非常清晰，它的功能是：在一个无限循环中，不断地去抓取USB摄像头拍摄的一帧图像，然后用model预测其分类及概率，最后再把预测结果叠加显示在GUI窗口中，类似于下面这样： 既然 cntkDemo.py 主要是为了测试model能不能正常跑，那么我在命令行以文字形式显示预测结果也是一样的啊，没有必要非得在GUI窗口中展示。 文章来源： https://www.codelast.com/ 于是我把程序改成了下面这样（完整程序）： import sys import os import numpy as np import cv2 import time import findEll import cntk_to_ell import modelHelper as mh def get_ell_predictor(modelConfig): &quot;&quot;&quot;Imports a model and returns an ELL.Predictor.&quot;&quot;&quot; return cntk_to_ell.predictor_from_cntk_model(modelConfig.model_files[0]) def main(): if (not os.path.exists(&#39;VGG16_ImageNet_Caffe.model&#39;)): print(&quot;Please download the &#39;VGG16_ImageNet_Caffe.model&#39; file, see README.md&quot;) sys.exit(1) # ModelConfig for VGG16 model from CNTK Model Gallery # Follow the instructions in README.md to download the model if you intend to use it. helper = mh.ModelHelper(&quot;VGG16ImageNet&quot;, [&quot;VGG16_ImageNet_Caffe.model&quot;], &quot;cntkVgg16ImageNetLabels.txt&quot;, scaleFactor=1.0) # Import the model model = get_ell_predictor(helper) # Save the model helper.save_ell_predictor_to_file(model, &quot;vgg16ImageNet.map&quot;) camera = 0 if (len(sys.argv) &gt; 1): camera = int(sys.argv[1]) # Start video capture device cap = cv2.VideoCapture(camera) while (True): print(&#39;Read a frame from camera...&#39;) ret, frame = cap.read() # Prepare the image to send to the model. # This involves scaling to the required input dimension and re-ordering from BGR to RGB data = helper.prepare_image_for_predictor(frame) # Get the model to classify the image, by returning a list of probabilities for the classes it can detect predictions = model.Predict(data) # Get the (at most) top 5 predictions that meet our threshold. This is returned as a list of tuples, # each with the text label and the prediction score. top5 = helper.get_top_n(predictions, 5) # Turn the top5 into a text string to display text = &quot;&quot;.join([str(element[0]) + &quot;(&quot; + str(int(100*element[1])) + &quot;%) &quot; for element in top5]) # Output the text on command line print(text) if __name__ == &quot;__main__&quot;: main() OpenBLAS : Your OS does not support AVX instructions. OpenBLAS is using Nehalem kernels as a fallback, which may give poorer performance. Read a frame from camera, time 1 Frame 1 saved to disk Read a frame from camera, time 2 Frame 2 saved to disk Read a frame from camera, time 3 Frame 3 saved to disk Read a frame from camera, time 4 Frame 4 saved to disk Read a frame from camera, time 5 Frame 5 saved to disk Loading… Selected CPU as the process wide default device. Finished loading. Pre-processing… Will not process Dropout – skipping this layer as irrelevant. Will not process Dropout – skipping this layer as irrelevant. Will not process Combine – skipping this layer as irrelevant. Convolution : 226x226x3 -&gt; 224x224x64 padding 1 ReLU : 224x224x64 -&gt; 226x226x64 padding 0 Convolution : 226x226x64 -&gt; 224x224x64 padding 1 ReLU : 224x224x64 -&gt; 224x224x64 padding 0 Pooling : 224x224x64 -&gt; 114x114x64 padding 0 Convolution : 114x114x64 -&gt; 112x112x128 padding 1 ReLU : 112x112x128 -&gt; 114x114x128 padding 0 Convolution : 114x114x128 -&gt; 112x112x128 padding 1 ReLU : 112x112x128 -&gt; 112x112x128 padding 0 Pooling : 112x112x128 -&gt; 58x58x128 padding 0 Convolution : 58x58x128 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 58x58x256 padding 0 Convolution : 58x58x256 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 58x58x256 padding 0 Convolution : 58x58x256 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 56x56x256 padding 0 Pooling : 56x56x256 -&gt; 30x30x256 padding 0 Convolution : 30x30x256 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 30x30x512 padding 0 Convolution : 30x30x512 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 30x30x512 padding 0 Convolution : 30x30x512 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 28x28x512 padding 0 Pooling : 28x28x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 14x14x512 padding 0 Pooling : 14x14x512 -&gt; 7x7x512 padding 0 linear : 7x7x512 -&gt; 1x1x4096 padding 0 ReLU : 1x1x4096 -&gt; 1x1x4096 padding 0 linear : 1x1x4096 -&gt; 1x1x4096 padding 0 ReLU : 1x1x4096 -&gt; 1x1x4096 padding 0 linear : 1x1x4096 -&gt; 1x1x1000 padding 0 Softmax : 1x1x1000 -&gt; 1x1x1000 padding 0 Finished pre-processing. Constructing equivalent ELL layers from CNTK… Converting layer conv1_1: Convolution(data: Tensor[3,224,224]) -&gt; Tensor[64,224,224] Converting layer relu1_1: ReLU(conv1_1: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer conv1_2: Convolution(relu1_1: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer relu1_2: ReLU(conv1_2: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer pool1: Pooling(relu1_2: Tensor[64,224,224]) -&gt; Tensor[64,112,112] Converting layer conv2_1: Convolution(pool1: Tensor[64,112,112]) -&gt; Tensor[128,112,112] Converting layer relu2_1: ReLU(conv2_1: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer conv2_2: Convolution(relu2_1: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer relu2_2: ReLU(conv2_2: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer pool2: Pooling(relu2_2: Tensor[128,112,112]) -&gt; Tensor[128,56,56] Converting layer conv3_1: Convolution(pool2: Tensor[128,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_1: ReLU(conv3_1: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer conv3_2: Convolution(relu3_1: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_2: ReLU(conv3_2: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer conv3_3: Convolution(relu3_2: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_3: ReLU(conv3_3: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer pool3: Pooling(relu3_3: Tensor[256,56,56]) -&gt; Tensor[256,28,28] Converting layer conv4_1: Convolution(pool3: Tensor[256,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_1: ReLU(conv4_1: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer conv4_2: Convolution(relu4_1: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_2: ReLU(conv4_2: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer conv4_3: Convolution(relu4_2: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_3: ReLU(conv4_3: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer pool4: Pooling(relu4_3: Tensor[512,28,28]) -&gt; Tensor[512,14,14] Converting layer conv5_1: Convolution(pool4: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_1: ReLU(conv5_1: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer conv5_2: Convolution(relu5_1: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_2: ReLU(conv5_2: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer conv5_3: Convolution(relu5_2: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_3: ReLU(conv5_3: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer pool5: Pooling(relu5_3: Tensor[512,14,14]) -&gt; Tensor[512,7,7] Converting layer fc6: linear(pool5: Tensor[512,7,7]) -&gt; Tensor[4096] Converting layer relu6: ReLU(fc6: Tensor[4096]) -&gt; Tensor[4096] Converting layer fc7: linear(drop6: Tensor[4096]) -&gt; Tensor[4096] Converting layer relu7: ReLU(fc7: Tensor[4096]) -&gt; Tensor[4096] Converting layer fc8: linear(drop7: Tensor[4096]) -&gt; Tensor[1000] Converting layer prob: Softmax(fc8: Tensor[1000]) -&gt; Tensor[1000] …Finished constructing ELL layers. lighter, light, igniter, ignitor(28%) lighter, light, igniter, ignitor(28%) lighter, light, igniter, ignitor(32%) lighter, light, igniter, ignitor(30%) ……" />
<link rel="canonical" href="http://www.jfox.info/2017/%E5%8E%9F%E5%88%9B%E6%89%A7%E8%A1%8Cell%E7%9A%84demo%E7%A8%8B%E5%BA%8Fcntkdemopy%E6%97%B6%E7%A8%8B%E5%BA%8F%E5%83%B5%E6%AD%BB%E7%9A%84%E9%97%AE%E9%A2%98.html" />
<meta property="og:url" content="http://www.jfox.info/2017/%E5%8E%9F%E5%88%9B%E6%89%A7%E8%A1%8Cell%E7%9A%84demo%E7%A8%8B%E5%BA%8Fcntkdemopy%E6%97%B6%E7%A8%8B%E5%BA%8F%E5%83%B5%E6%AD%BB%E7%9A%84%E9%97%AE%E9%A2%98.html" />
<meta property="og:site_name" content="Java面试" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-01-01T15:57:52+00:00" />
<script type="application/ld+json">
{"description":"OS：Ubuntu 14.04 在台式机上执行ELL的demo程序 cntkDemo.py 时，可能会遇到程序僵死的问题。 cntkDemo.py 这个程序会调用OpenCV，在一个GUI窗口中显示USB摄像头拍摄的实时视频流，而僵死的现象正是：执行到弹出GUI窗口显示摄像头拍摄的视频流的代码的时候，程序进入僵死状态，不能执行后续逻辑。此时，只能Ctrl+C终止掉程序。 我的Ubuntu 14.04是一台老爷机，性能非常差，我觉得这有可能程序僵死的原因之一？我试了几次都是这样，于是我打算换一个思路来跑这个demo，不再纠结于解决窗口僵死的问题。 文章来源： https://www.codelast.com/ 先来看一下原版的 cntkDemo.py 部分代码： while (True): # Grab next frame ret, frame = cap.read() # Prepare the image to send to the model. # This involves scaling to the required input dimension and re-ordering from BGR to RGB data = helper.prepare_image_for_predictor(frame) # Get the model to classify the image, by returning a list of probabilities for the classes it can detect predictions = model.Predict(data) # Get the (at most) top 5 predictions that meet our threshold. This is returned as a list of tuples, # each with the text label and the prediction score. top5 = helper.get_top_n(predictions, 5) # Turn the top5 into a text string to display text = &quot;&quot;.join([str(element[0]) + &quot;(&quot; + str(int(100*element[1])) + &quot;%) &quot; for element in top5]) # Draw the text on the frame frameToShow = frame helper.draw_label(frameToShow, text) helper.draw_fps(frameToShow) # Show the new frame cv2.imshow(&#39;frame&#39;, frameToShow) # Wait for Esc key if cv2.waitKey(1) &amp; 0xFF == 27: break 这段代码的注释非常清晰，它的功能是：在一个无限循环中，不断地去抓取USB摄像头拍摄的一帧图像，然后用model预测其分类及概率，最后再把预测结果叠加显示在GUI窗口中，类似于下面这样： 既然 cntkDemo.py 主要是为了测试model能不能正常跑，那么我在命令行以文字形式显示预测结果也是一样的啊，没有必要非得在GUI窗口中展示。 文章来源： https://www.codelast.com/ 于是我把程序改成了下面这样（完整程序）： import sys import os import numpy as np import cv2 import time import findEll import cntk_to_ell import modelHelper as mh def get_ell_predictor(modelConfig): &quot;&quot;&quot;Imports a model and returns an ELL.Predictor.&quot;&quot;&quot; return cntk_to_ell.predictor_from_cntk_model(modelConfig.model_files[0]) def main(): if (not os.path.exists(&#39;VGG16_ImageNet_Caffe.model&#39;)): print(&quot;Please download the &#39;VGG16_ImageNet_Caffe.model&#39; file, see README.md&quot;) sys.exit(1) # ModelConfig for VGG16 model from CNTK Model Gallery # Follow the instructions in README.md to download the model if you intend to use it. helper = mh.ModelHelper(&quot;VGG16ImageNet&quot;, [&quot;VGG16_ImageNet_Caffe.model&quot;], &quot;cntkVgg16ImageNetLabels.txt&quot;, scaleFactor=1.0) # Import the model model = get_ell_predictor(helper) # Save the model helper.save_ell_predictor_to_file(model, &quot;vgg16ImageNet.map&quot;) camera = 0 if (len(sys.argv) &gt; 1): camera = int(sys.argv[1]) # Start video capture device cap = cv2.VideoCapture(camera) while (True): print(&#39;Read a frame from camera...&#39;) ret, frame = cap.read() # Prepare the image to send to the model. # This involves scaling to the required input dimension and re-ordering from BGR to RGB data = helper.prepare_image_for_predictor(frame) # Get the model to classify the image, by returning a list of probabilities for the classes it can detect predictions = model.Predict(data) # Get the (at most) top 5 predictions that meet our threshold. This is returned as a list of tuples, # each with the text label and the prediction score. top5 = helper.get_top_n(predictions, 5) # Turn the top5 into a text string to display text = &quot;&quot;.join([str(element[0]) + &quot;(&quot; + str(int(100*element[1])) + &quot;%) &quot; for element in top5]) # Output the text on command line print(text) if __name__ == &quot;__main__&quot;: main() OpenBLAS : Your OS does not support AVX instructions. OpenBLAS is using Nehalem kernels as a fallback, which may give poorer performance. Read a frame from camera, time 1 Frame 1 saved to disk Read a frame from camera, time 2 Frame 2 saved to disk Read a frame from camera, time 3 Frame 3 saved to disk Read a frame from camera, time 4 Frame 4 saved to disk Read a frame from camera, time 5 Frame 5 saved to disk Loading… Selected CPU as the process wide default device. Finished loading. Pre-processing… Will not process Dropout – skipping this layer as irrelevant. Will not process Dropout – skipping this layer as irrelevant. Will not process Combine – skipping this layer as irrelevant. Convolution : 226x226x3 -&gt; 224x224x64 padding 1 ReLU : 224x224x64 -&gt; 226x226x64 padding 0 Convolution : 226x226x64 -&gt; 224x224x64 padding 1 ReLU : 224x224x64 -&gt; 224x224x64 padding 0 Pooling : 224x224x64 -&gt; 114x114x64 padding 0 Convolution : 114x114x64 -&gt; 112x112x128 padding 1 ReLU : 112x112x128 -&gt; 114x114x128 padding 0 Convolution : 114x114x128 -&gt; 112x112x128 padding 1 ReLU : 112x112x128 -&gt; 112x112x128 padding 0 Pooling : 112x112x128 -&gt; 58x58x128 padding 0 Convolution : 58x58x128 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 58x58x256 padding 0 Convolution : 58x58x256 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 58x58x256 padding 0 Convolution : 58x58x256 -&gt; 56x56x256 padding 1 ReLU : 56x56x256 -&gt; 56x56x256 padding 0 Pooling : 56x56x256 -&gt; 30x30x256 padding 0 Convolution : 30x30x256 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 30x30x512 padding 0 Convolution : 30x30x512 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 30x30x512 padding 0 Convolution : 30x30x512 -&gt; 28x28x512 padding 1 ReLU : 28x28x512 -&gt; 28x28x512 padding 0 Pooling : 28x28x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 16x16x512 padding 0 Convolution : 16x16x512 -&gt; 14x14x512 padding 1 ReLU : 14x14x512 -&gt; 14x14x512 padding 0 Pooling : 14x14x512 -&gt; 7x7x512 padding 0 linear : 7x7x512 -&gt; 1x1x4096 padding 0 ReLU : 1x1x4096 -&gt; 1x1x4096 padding 0 linear : 1x1x4096 -&gt; 1x1x4096 padding 0 ReLU : 1x1x4096 -&gt; 1x1x4096 padding 0 linear : 1x1x4096 -&gt; 1x1x1000 padding 0 Softmax : 1x1x1000 -&gt; 1x1x1000 padding 0 Finished pre-processing. Constructing equivalent ELL layers from CNTK… Converting layer conv1_1: Convolution(data: Tensor[3,224,224]) -&gt; Tensor[64,224,224] Converting layer relu1_1: ReLU(conv1_1: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer conv1_2: Convolution(relu1_1: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer relu1_2: ReLU(conv1_2: Tensor[64,224,224]) -&gt; Tensor[64,224,224] Converting layer pool1: Pooling(relu1_2: Tensor[64,224,224]) -&gt; Tensor[64,112,112] Converting layer conv2_1: Convolution(pool1: Tensor[64,112,112]) -&gt; Tensor[128,112,112] Converting layer relu2_1: ReLU(conv2_1: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer conv2_2: Convolution(relu2_1: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer relu2_2: ReLU(conv2_2: Tensor[128,112,112]) -&gt; Tensor[128,112,112] Converting layer pool2: Pooling(relu2_2: Tensor[128,112,112]) -&gt; Tensor[128,56,56] Converting layer conv3_1: Convolution(pool2: Tensor[128,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_1: ReLU(conv3_1: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer conv3_2: Convolution(relu3_1: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_2: ReLU(conv3_2: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer conv3_3: Convolution(relu3_2: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer relu3_3: ReLU(conv3_3: Tensor[256,56,56]) -&gt; Tensor[256,56,56] Converting layer pool3: Pooling(relu3_3: Tensor[256,56,56]) -&gt; Tensor[256,28,28] Converting layer conv4_1: Convolution(pool3: Tensor[256,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_1: ReLU(conv4_1: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer conv4_2: Convolution(relu4_1: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_2: ReLU(conv4_2: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer conv4_3: Convolution(relu4_2: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer relu4_3: ReLU(conv4_3: Tensor[512,28,28]) -&gt; Tensor[512,28,28] Converting layer pool4: Pooling(relu4_3: Tensor[512,28,28]) -&gt; Tensor[512,14,14] Converting layer conv5_1: Convolution(pool4: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_1: ReLU(conv5_1: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer conv5_2: Convolution(relu5_1: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_2: ReLU(conv5_2: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer conv5_3: Convolution(relu5_2: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer relu5_3: ReLU(conv5_3: Tensor[512,14,14]) -&gt; Tensor[512,14,14] Converting layer pool5: Pooling(relu5_3: Tensor[512,14,14]) -&gt; Tensor[512,7,7] Converting layer fc6: linear(pool5: Tensor[512,7,7]) -&gt; Tensor[4096] Converting layer relu6: ReLU(fc6: Tensor[4096]) -&gt; Tensor[4096] Converting layer fc7: linear(drop6: Tensor[4096]) -&gt; Tensor[4096] Converting layer relu7: ReLU(fc7: Tensor[4096]) -&gt; Tensor[4096] Converting layer fc8: linear(drop7: Tensor[4096]) -&gt; Tensor[1000] Converting layer prob: Softmax(fc8: Tensor[1000]) -&gt; Tensor[1000] …Finished constructing ELL layers. lighter, light, igniter, ignitor(28%) lighter, light, igniter, ignitor(28%) lighter, light, igniter, ignitor(32%) lighter, light, igniter, ignitor(30%) ……","@type":"BlogPosting","url":"http://www.jfox.info/2017/%E5%8E%9F%E5%88%9B%E6%89%A7%E8%A1%8Cell%E7%9A%84demo%E7%A8%8B%E5%BA%8Fcntkdemopy%E6%97%B6%E7%A8%8B%E5%BA%8F%E5%83%B5%E6%AD%BB%E7%9A%84%E9%97%AE%E9%A2%98.html","headline":"[原创] 执行ELL的demo程序cntkDemo.py时程序僵死的问题","dateModified":"2017-01-01T15:57:52+00:00","datePublished":"2017-01-01T15:57:52+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.jfox.info/2017/%E5%8E%9F%E5%88%9B%E6%89%A7%E8%A1%8Cell%E7%9A%84demo%E7%A8%8B%E5%BA%8Fcntkdemopy%E6%97%B6%E7%A8%8B%E5%BA%8F%E5%83%B5%E6%AD%BB%E7%9A%84%E9%97%AE%E9%A2%98.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://www.jfox.info/feed.xml" title="Java面试" /></head><body><header class="site-header" role="banner">
  <div class="wrapper"><a class="site-title" rel="author" href="/">Java面试</a><nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z" />
          </svg>
        </span>
      </label>

      <div class="trigger"><a class="page-link" href="/about/">About</a></div>
    </nav></div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">[原创] 执行ELL的demo程序cntkDemo.py时程序僵死的问题</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2017-01-01T15:57:52+00:00" itemprop="datePublished">Jan 1, 2017
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    
<p>OS：Ubuntu 14.04</p>

<p>在台式机上执行ELL的demo程序 cntkDemo.py 时，可能会遇到程序僵死的问题。 
cntkDemo.py 这个程序会调用OpenCV，在一个GUI窗口中显示USB摄像头拍摄的实时视频流，而僵死的现象正是：执行到弹出GUI窗口显示摄像头拍摄的视频流的代码的时候，程序进入僵死状态，不能执行后续逻辑。此时，只能Ctrl+C终止掉程序。 
我的Ubuntu 14.04是一台老爷机，性能非常差，我觉得这有可能程序僵死的原因之一？我试了几次都是这样，于是我打算换一个思路来跑这个demo，不再纠结于解决窗口僵死的问题。 
文章来源： <a href="https://www.jfox.info/go.php?url=https://www.codelast.com/"> https://www.codelast.com/ </a>
先来看一下原版的 cntkDemo.py 部分代码：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    while (True):
        # Grab next frame
        ret, frame = cap.read()

        # Prepare the image to send to the model.
        # This involves scaling to the required input dimension and re-ordering from BGR to RGB
        data = helper.prepare_image_for_predictor(frame)

        # Get the model to classify the image, by returning a list of probabilities for the classes it can detect
        predictions = model.Predict(data)

        # Get the (at most) top 5 predictions that meet our threshold. This is returned as a list of tuples,
        # each with the text label and the prediction score.
        top5 = helper.get_top_n(predictions, 5)

        # Turn the top5 into a text string to display
        text = "".join([str(element[0]) + "(" + str(int(100*element[1])) + "%)  " for element in top5])

        # Draw the text on the frame
        frameToShow = frame
        helper.draw_label(frameToShow, text)
        helper.draw_fps(frameToShow)

        # Show the new frame
        cv2.imshow('frame', frameToShow)

        # Wait for Esc key
        if cv2.waitKey(1) &amp; 0xFF == 27:
            break
</code></pre></div></div>

<p>这段代码的注释非常清晰，它的功能是：在一个无限循环中，不断地去抓取USB摄像头拍摄的一帧图像，然后用model预测其分类及概率，最后再把预测结果叠加显示在GUI窗口中，类似于下面这样：</p>

<p><img src="a221a9d.png" alt="" /></p>

<p>既然 cntkDemo.py 主要是为了测试model能不能正常跑，那么我在命令行以文字形式显示预测结果也是一样的啊，没有必要非得在GUI窗口中展示。 
文章来源： <a href="https://www.jfox.info/go.php?url=https://www.codelast.com/"> https://www.codelast.com/ </a>
于是我把程序改成了下面这样（完整程序）：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">import</span> <span class="n">sys</span>
<span class="n">import</span> <span class="n">os</span>
<span class="n">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">import</span> <span class="n">cv2</span>
<span class="n">import</span> <span class="n">time</span>

<span class="n">import</span> <span class="n">findEll</span>
<span class="n">import</span> <span class="n">cntk_to_ell</span>
<span class="n">import</span> <span class="n">modelHelper</span> <span class="k">as</span> <span class="n">mh</span>

<span class="n">def</span> <span class="n">get_ell_predictor</span><span class="p">(</span><span class="n">modelConfig</span><span class="p">):</span>
    <span class="s2">"""Imports a model and returns an ELL.Predictor."""</span>
    <span class="n">return</span> <span class="n">cntk_to_ell</span><span class="p">.</span><span class="n">predictor_from_cntk_model</span><span class="p">(</span><span class="n">modelConfig</span><span class="p">.</span><span class="n">model_files</span><span class="p">[</span><span class="m">0</span><span class="p">])</span>

<span class="n">def</span> <span class="n">main</span><span class="p">():</span>

    <span class="k">if</span> <span class="p">(</span><span class="k">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nb">exists</span><span class="p">(</span><span class="s1">'VGG16_ImageNet_Caffe.model'</span><span class="p">)):</span>
        <span class="n">print</span><span class="p">(</span><span class="s2">"Please download the 'VGG16_ImageNet_Caffe.model' file, see README.md"</span><span class="p">)</span>
        <span class="n">sys</span><span class="p">.</span><span class="k">exit</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
        
    <span class="p">#</span> <span class="n">ModelConfig</span> <span class="n">for</span> <span class="n">VGG16</span> <span class="k">model</span> <span class="k">from</span> <span class="n">CNTK</span> <span class="k">Model</span> <span class="n">Gallery</span>
    <span class="p">#</span> <span class="n">Follow</span> <span class="n">the</span> <span class="n">instructions</span> <span class="k">in</span> <span class="n">README</span><span class="p">.</span><span class="n">md</span> <span class="k">to</span> <span class="n">download</span> <span class="n">the</span> <span class="k">model</span> <span class="k">if</span> <span class="n">you</span> <span class="n">intend</span> <span class="k">to</span> <span class="n">use</span> <span class="n">it</span><span class="p">.</span>
    <span class="n">helper</span> <span class="p">=</span> <span class="n">mh</span><span class="p">.</span><span class="n">ModelHelper</span><span class="p">(</span><span class="s2">"VGG16ImageNet"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"VGG16_ImageNet_Caffe.model"</span><span class="p">],</span> <span class="s2">"cntkVgg16ImageNetLabels.txt"</span><span class="p">,</span> <span class="n">scaleFactor</span><span class="p">=</span><span class="m">1.0</span><span class="p">)</span>

    <span class="p">#</span> <span class="n">Import</span> <span class="n">the</span> <span class="k">model</span>
    <span class="k">model</span> <span class="p">=</span> <span class="n">get_ell_predictor</span><span class="p">(</span><span class="n">helper</span><span class="p">)</span>

    <span class="p">#</span> <span class="n">Save</span> <span class="n">the</span> <span class="k">model</span>
    <span class="n">helper</span><span class="p">.</span><span class="n">save_ell_predictor_to_file</span><span class="p">(</span><span class="k">model</span><span class="p">,</span> <span class="s2">"vgg16ImageNet.map"</span><span class="p">)</span>

    <span class="n">camera</span> <span class="p">=</span> <span class="m">0</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">)</span> <span class="p">&gt;</span> <span class="m">1</span><span class="p">):</span>
        <span class="n">camera</span> <span class="p">=</span> <span class="n">int</span><span class="p">(</span><span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="m">1</span><span class="p">])</span> 

    <span class="p">#</span> <span class="n">Start</span> <span class="n">video</span> <span class="n">capture</span> <span class="n">device</span>
    <span class="n">cap</span> <span class="p">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">camera</span><span class="p">)</span>

    <span class="k">while</span> <span class="p">(</span><span class="nb">True</span><span class="p">):</span>
        <span class="n">print</span><span class="p">(</span><span class="s1">'Read a frame from camera...'</span><span class="p">)</span>
        <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="p">=</span> <span class="n">cap</span><span class="p">.</span><span class="nb">read</span><span class="p">()</span>

        <span class="p">#</span> <span class="n">Prepare</span> <span class="n">the</span> <span class="n">image</span> <span class="k">to</span> <span class="nf">send</span> <span class="k">to</span> <span class="n">the</span> <span class="k">model</span><span class="p">.</span>
        <span class="p">#</span> <span class="n">This</span> <span class="n">involves</span> <span class="n">scaling</span> <span class="k">to</span> <span class="n">the</span> <span class="n">required</span> <span class="n">input</span> <span class="n">dimension</span> <span class="k">and</span> <span class="n">re</span><span class="p">-</span><span class="n">ordering</span> <span class="k">from</span> <span class="n">BGR</span> <span class="k">to</span> <span class="n">RGB</span>
        <span class="n">data</span> <span class="p">=</span> <span class="n">helper</span><span class="p">.</span><span class="n">prepare_image_for_predictor</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

        <span class="p">#</span> <span class="n">Get</span> <span class="n">the</span> <span class="k">model</span> <span class="k">to</span> <span class="n">classify</span> <span class="n">the</span> <span class="n">image</span><span class="p">,</span> <span class="n">by</span> <span class="n">returning</span> <span class="n">a</span> <span class="k">list</span> <span class="k">of</span> <span class="n">probabilities</span> <span class="n">for</span> <span class="n">the</span> <span class="n">classes</span> <span class="n">it</span> <span class="n">can</span> <span class="n">detect</span>
        <span class="n">predictions</span> <span class="p">=</span> <span class="k">model</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="p">#</span> <span class="n">Get</span> <span class="n">the</span> <span class="p">(</span><span class="n">at</span> <span class="n">most</span><span class="p">)</span> <span class="n">top</span> <span class="m">5</span> <span class="n">predictions</span> <span class="n">that</span> <span class="n">meet</span> <span class="n">our</span> <span class="n">threshold</span><span class="p">.</span> <span class="n">This</span> <span class="n">is</span> <span class="n">returned</span> <span class="k">as</span> <span class="n">a</span> <span class="k">list</span> <span class="k">of</span> <span class="n">tuples</span><span class="p">,</span>
        <span class="p">#</span> <span class="n">each</span> <span class="k">with</span> <span class="n">the</span> <span class="n">text</span> <span class="n">label</span> <span class="k">and</span> <span class="n">the</span> <span class="n">prediction</span> <span class="n">score</span><span class="p">.</span>
        <span class="n">top5</span> <span class="p">=</span> <span class="n">helper</span><span class="p">.</span><span class="n">get_top_n</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="m">5</span><span class="p">)</span>

        <span class="p">#</span> <span class="n">Turn</span> <span class="n">the</span> <span class="n">top5</span> <span class="n">into</span> <span class="n">a</span> <span class="n">text</span> <span class="k">string</span> <span class="k">to</span> <span class="n">display</span>
        <span class="n">text</span> <span class="p">=</span> <span class="s2">""</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="n">str</span><span class="p">(</span><span class="n">element</span><span class="p">[</span><span class="m">0</span><span class="p">])</span> <span class="p">+</span> <span class="s2">"("</span> <span class="p">+</span> <span class="n">str</span><span class="p">(</span><span class="n">int</span><span class="p">(</span><span class="m">100</span><span class="p">*</span><span class="n">element</span><span class="p">[</span><span class="m">1</span><span class="p">]))</span> <span class="p">+</span> <span class="s2">"%)  "</span> <span class="n">for</span> <span class="n">element</span> <span class="k">in</span> <span class="n">top5</span><span class="p">])</span>

        <span class="p">#</span> <span class="n">Output</span> <span class="n">the</span> <span class="n">text</span> <span class="n">on</span> <span class="nf">command</span> <span class="n">line</span>
        <span class="n">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="p">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<p>OpenBLAS : Your OS does not support AVX instructions. OpenBLAS is using Nehalem kernels as a fallback, which may give poorer performance.</p>

<p>Read a frame from camera, time 1</p>

<p>Frame 1 saved to disk</p>

<p>Read a frame from camera, time 2</p>

<p>Frame 2 saved to disk</p>

<p>Read a frame from camera, time 3</p>

<p>Frame 3 saved to disk</p>

<p>Read a frame from camera, time 4</p>

<p>Frame 4 saved to disk</p>

<p>Read a frame from camera, time 5</p>

<p>Frame 5 saved to disk</p>

<p>Loading…</p>

<p>Selected CPU as the process wide default device.</p>

<p>Finished loading.</p>

<p>Pre-processing…</p>

<p>Will not process Dropout – skipping this layer as irrelevant.</p>

<p>Will not process Dropout – skipping this layer as irrelevant.</p>

<p>Will not process Combine – skipping this layer as irrelevant.</p>

<table>
  <tbody>
    <tr>
      <td>Convolution :  226x226x3  -&gt;  224x224x64</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  224x224x64  -&gt;  226x226x64</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  226x226x64  -&gt;  224x224x64</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  224x224x64  -&gt;  224x224x64</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Pooling :  224x224x64  -&gt;  114x114x64</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  114x114x64  -&gt;  112x112x128</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  112x112x128  -&gt;  114x114x128</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  114x114x128  -&gt;  112x112x128</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  112x112x128  -&gt;  112x112x128</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Pooling :  112x112x128  -&gt;  58x58x128</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  58x58x128  -&gt;  56x56x256</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  56x56x256  -&gt;  58x58x256</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  58x58x256  -&gt;  56x56x256</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  56x56x256  -&gt;  58x58x256</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  58x58x256  -&gt;  56x56x256</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  56x56x256  -&gt;  56x56x256</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Pooling :  56x56x256  -&gt;  30x30x256</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  30x30x256  -&gt;  28x28x512</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  28x28x512  -&gt;  30x30x512</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  30x30x512  -&gt;  28x28x512</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  28x28x512  -&gt;  30x30x512</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  30x30x512  -&gt;  28x28x512</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  28x28x512  -&gt;  28x28x512</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Pooling :  28x28x512  -&gt;  16x16x512</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  16x16x512  -&gt;  14x14x512</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  14x14x512  -&gt;  16x16x512</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  16x16x512  -&gt;  14x14x512</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  14x14x512  -&gt;  16x16x512</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Convolution :  16x16x512  -&gt;  14x14x512</td>
      <td>padding  1</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  14x14x512  -&gt;  14x14x512</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Pooling :  14x14x512  -&gt;  7x7x512</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>linear :  7x7x512  -&gt;  1x1x4096</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  1x1x4096  -&gt;  1x1x4096</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>linear :  1x1x4096  -&gt;  1x1x4096</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ReLU :  1x1x4096  -&gt;  1x1x4096</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>linear :  1x1x4096  -&gt;  1x1x1000</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Softmax :  1x1x1000  -&gt;  1x1x1000</td>
      <td>padding  0</td>
    </tr>
  </tbody>
</table>

<p>Finished pre-processing.</p>

<p>Constructing equivalent ELL layers from CNTK…</p>

<p>Converting layer  conv1_1: Convolution(data: Tensor[3,224,224]) -&gt; Tensor[64,224,224]</p>

<p>Converting layer  relu1_1: ReLU(conv1_1: Tensor[64,224,224]) -&gt; Tensor[64,224,224]</p>

<p>Converting layer  conv1_2: Convolution(relu1_1: Tensor[64,224,224]) -&gt; Tensor[64,224,224]</p>

<p>Converting layer  relu1_2: ReLU(conv1_2: Tensor[64,224,224]) -&gt; Tensor[64,224,224]</p>

<p>Converting layer  pool1: Pooling(relu1_2: Tensor[64,224,224]) -&gt; Tensor[64,112,112]</p>

<p>Converting layer  conv2_1: Convolution(pool1: Tensor[64,112,112]) -&gt; Tensor[128,112,112]</p>

<p>Converting layer  relu2_1: ReLU(conv2_1: Tensor[128,112,112]) -&gt; Tensor[128,112,112]</p>

<p>Converting layer  conv2_2: Convolution(relu2_1: Tensor[128,112,112]) -&gt; Tensor[128,112,112]</p>

<p>Converting layer  relu2_2: ReLU(conv2_2: Tensor[128,112,112]) -&gt; Tensor[128,112,112]</p>

<p>Converting layer  pool2: Pooling(relu2_2: Tensor[128,112,112]) -&gt; Tensor[128,56,56]</p>

<p>Converting layer  conv3_1: Convolution(pool2: Tensor[128,56,56]) -&gt; Tensor[256,56,56]</p>

<p>Converting layer  relu3_1: ReLU(conv3_1: Tensor[256,56,56]) -&gt; Tensor[256,56,56]</p>

<p>Converting layer  conv3_2: Convolution(relu3_1: Tensor[256,56,56]) -&gt; Tensor[256,56,56]</p>

<p>Converting layer  relu3_2: ReLU(conv3_2: Tensor[256,56,56]) -&gt; Tensor[256,56,56]</p>

<p>Converting layer  conv3_3: Convolution(relu3_2: Tensor[256,56,56]) -&gt; Tensor[256,56,56]</p>

<p>Converting layer  relu3_3: ReLU(conv3_3: Tensor[256,56,56]) -&gt; Tensor[256,56,56]</p>

<p>Converting layer  pool3: Pooling(relu3_3: Tensor[256,56,56]) -&gt; Tensor[256,28,28]</p>

<p>Converting layer  conv4_1: Convolution(pool3: Tensor[256,28,28]) -&gt; Tensor[512,28,28]</p>

<p>Converting layer  relu4_1: ReLU(conv4_1: Tensor[512,28,28]) -&gt; Tensor[512,28,28]</p>

<p>Converting layer  conv4_2: Convolution(relu4_1: Tensor[512,28,28]) -&gt; Tensor[512,28,28]</p>

<p>Converting layer  relu4_2: ReLU(conv4_2: Tensor[512,28,28]) -&gt; Tensor[512,28,28]</p>

<p>Converting layer  conv4_3: Convolution(relu4_2: Tensor[512,28,28]) -&gt; Tensor[512,28,28]</p>

<p>Converting layer  relu4_3: ReLU(conv4_3: Tensor[512,28,28]) -&gt; Tensor[512,28,28]</p>

<p>Converting layer  pool4: Pooling(relu4_3: Tensor[512,28,28]) -&gt; Tensor[512,14,14]</p>

<p>Converting layer  conv5_1: Convolution(pool4: Tensor[512,14,14]) -&gt; Tensor[512,14,14]</p>

<p>Converting layer  relu5_1: ReLU(conv5_1: Tensor[512,14,14]) -&gt; Tensor[512,14,14]</p>

<p>Converting layer  conv5_2: Convolution(relu5_1: Tensor[512,14,14]) -&gt; Tensor[512,14,14]</p>

<p>Converting layer  relu5_2: ReLU(conv5_2: Tensor[512,14,14]) -&gt; Tensor[512,14,14]</p>

<p>Converting layer  conv5_3: Convolution(relu5_2: Tensor[512,14,14]) -&gt; Tensor[512,14,14]</p>

<p>Converting layer  relu5_3: ReLU(conv5_3: Tensor[512,14,14]) -&gt; Tensor[512,14,14]</p>

<p>Converting layer  pool5: Pooling(relu5_3: Tensor[512,14,14]) -&gt; Tensor[512,7,7]</p>

<p>Converting layer  fc6: linear(pool5: Tensor[512,7,7]) -&gt; Tensor[4096]</p>

<p>Converting layer  relu6: ReLU(fc6: Tensor[4096]) -&gt; Tensor[4096]</p>

<p>Converting layer  fc7: linear(drop6: Tensor[4096]) -&gt; Tensor[4096]</p>

<p>Converting layer  relu7: ReLU(fc7: Tensor[4096]) -&gt; Tensor[4096]</p>

<p>Converting layer  fc8: linear(drop7: Tensor[4096]) -&gt; Tensor[1000]</p>

<p>Converting layer  prob: Softmax(fc8: Tensor[1000]) -&gt; Tensor[1000]</p>

<p>…Finished constructing ELL layers.</p>

<p>lighter, light, igniter, ignitor(28%)</p>

<p>lighter, light, igniter, ignitor(28%)</p>

<p>lighter, light, igniter, ignitor(32%)</p>

<p>lighter, light, igniter, ignitor(30%)</p>

<p>……</p>

  </div><div style="width:300px;height:250px;float:left;">
    <!-- 300_250_1 -->
    <ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196"
        data-ad-slot="4142158067"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<div style="width:300px;height:250px;float:left;">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- 300-250-2 -->
    <ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196"
        data-ad-slot="5618891265"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div><a class="u-url" href="/2017/%E5%8E%9F%E5%88%9B%E6%89%A7%E8%A1%8Cell%E7%9A%84demo%E7%A8%8B%E5%BA%8Fcntkdemopy%E6%97%B6%E7%A8%8B%E5%BA%8F%E5%83%B5%E6%AD%BB%E7%9A%84%E9%97%AE%E9%A2%98.html" hidden></a>
</article>
<div class="PageNavigation">
  
  <a class="prev" href="/2017/%E7%9C%8B%E9%80%8Fspringmvc%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E8%B7%B5springmvc%E7%BB%84%E4%BB%B6%E5%88%86%E6%9E%90.html">&laquo; 看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析</a>
  
  
  <a class="next" href="/2017/lombok%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95-2.html">lombok的简单介绍和使用方法 &raquo;</a>
  
</div>
<div class="sfix"><!-- 240x600 -->
<div class="fixedme">
    <ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9477174171188196" data-ad-slot="9597600460"
        data-ad-format="auto"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<script type="text/javascript">
    function offset(target) {
        var top = 0,
            left = 0

        while (target.offsetParent) {
            top += target.offsetTop
            left += target.offsetLeft
            target = target.offsetParent
        }

        return {
            top: top,
            left: left,
        }
    }
    window.onload = function () {
        var e = document.getElementsByClassName('sfix')[0];
        e.offset = offset(e);

        window.onscroll = function (event) {
            var e = document.getElementsByClassName('sfix')[0];
            if (window.pageYOffset && e.offset && window.pageYOffset > e.offset.top) {
                e.style.position = 'fixed';
                e.style.left = e.offset.left + 'px';
                e.style.right = 'auto';


            } else {
                e.style.position = 'absolute';
                e.style.left = 'auto';
                e.style.right = '-240px';

            }
        }
    }

</script></div>

<style>
  .wrapper {
    position: relative;
  }

  .sfix {
    position: absolute;
    top: 0;
    right: -240px;
    width: 240px;
    height: 600px;
  }

  .PageNavigation {
    font-size: 14px;
    display: block;
    width: auto;
    overflow: hidden;
    clear: both;
  }

  .PageNavigation a {
    display: block;
    width: 50%;
    float: left;
    margin: 1em 0;
  }

  .PageNavigation .next {
    text-align: right;
    float: right;
  }
</style>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Java面试</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Java面试</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
