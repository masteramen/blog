<!-- build time:Sat Oct 27 2018 21:00:20 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next muse use-motion"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="description" content="这里，我们主要关注最主要的 2 个地方的初始化，首先是 TaskScheduler 的创建初始化。// Create and start the scheduler     val (sched, ts) = SparkContext.createTaskScheduler(this, master)     _schedulerBackend = sched     _taskScheduler"><meta name="keywords" content="JAVA,面试"><meta property="og:type" content="article"><meta property="og:title" content="spark源码分析之SparkContext初始化一"><meta property="og:url" content="http://www.jfox.info/2017/spark源码分析之sparkcontext初始化一.html"><meta property="og:site_name" content="java面试"><meta property="og:description" content="这里，我们主要关注最主要的 2 个地方的初始化，首先是 TaskScheduler 的创建初始化。// Create and start the scheduler     val (sched, ts) = SparkContext.createTaskScheduler(this, master)     _schedulerBackend = sched     _taskScheduler"><meta property="og:locale" content="zh_CN"><meta property="og:updated_time" content="2018-10-27T12:56:21.926Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="spark源码分析之SparkContext初始化一"><meta name="twitter:description" content="这里，我们主要关注最主要的 2 个地方的初始化，首先是 TaskScheduler 的创建初始化。// Create and start the scheduler     val (sched, ts) = SparkContext.createTaskScheduler(this, master)     _schedulerBackend = sched     _taskScheduler"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.jfox.info/2017/spark源码分析之sparkcontext初始化一.html"><title>spark源码分析之SparkContext初始化一 | java面试</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh_CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">java面试</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.jfox.info/2017/spark源码分析之sparkcontext初始化一.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Hello"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="java面试"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">spark源码分析之SparkContext初始化一</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-01T23:54:01+08:00">2017-01-01</time></span></div></header><div class="post-body" itemprop="articleBody"><p>这里，我们主要关注最主要的 2 个地方的初始化，首先是 TaskScheduler 的创建初始化。</p><pre><code>// Create and start the scheduler
    val (sched, ts) = SparkContext.createTaskScheduler(this, master)
    _schedulerBackend = sched
    _taskScheduler = ts
    _dagScheduler = new DAGScheduler(this)
    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
</code></pre><p>这里我们发现还会初始化 SchedulerBackend，这里我们继续看 createTaskScheduler 方法</p><pre><code>case SPARK_REGEX(sparkUrl) =&gt;
        val scheduler = new TaskSchedulerImpl(sc)
        val masterUrls = sparkUrl.split(&quot;,&quot;).map(&quot;spark://&quot; + _)
        val backend = new SparkDeploySchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        (backend, scheduler)
</code></pre><p>首先创建 TaskSchedulerImpl，里面有 2 个比较重要的变量</p><pre><code>// Listener object to pass upcalls into
  var dagScheduler: DAGScheduler = null
  var backend: SchedulerBackend = null
</code></pre><p>然后看创建 SparkDeploySchedulerBackend，最主要的方法在下面，因为会之后的程序会调用这个方法</p><pre><code>override def start() {
    super.start()
    // The endpoint for executors to talk to us
    val driverUrl = rpcEnv.uriOf(SparkEnv.driverActorSystemName,
      RpcAddress(sc.conf.get(&quot;spark.driver.host&quot;), sc.conf.get(&quot;spark.driver.port&quot;).toInt),
      CoarseGrainedSchedulerBackend.ENDPOINT_NAME)
    val args = Seq(
      &quot;--driver-url&quot;, driverUrl,
      &quot;--executor-id&quot;, &quot;{%raw%}{{{%endraw%}EXECUTOR_ID{% raw%}}}{% endraw%}&quot;,
      &quot;--hostname&quot;, &quot;{%raw%}{{{%endraw%}HOSTNAME{% raw%}}}{% endraw%}&quot;,
      &quot;--cores&quot;, &quot;{%raw%}{{{%endraw%}CORES{% raw%}}}{% endraw%}&quot;,
      &quot;--app-id&quot;, &quot;{%raw%}{{{%endraw%}APP_ID{% raw%}}}{% endraw%}&quot;,
      &quot;--worker-url&quot;, &quot;{%raw%}{{{%endraw%}WORKER_URL{% raw%}}}{% endraw%}&quot;)
    val extraJavaOpts = sc.conf.getOption(&quot;spark.executor.extraJavaOptions&quot;)
      .map(Utils.splitCommandString).getOrElse(Seq.empty)
    val classPathEntries = sc.conf.getOption(&quot;spark.executor.extraClassPath&quot;)
      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)
    val libraryPathEntries = sc.conf.getOption(&quot;spark.executor.extraLibraryPath&quot;)
      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)
    // When testing, expose the parent class path to the child. This is processed by
    // compute-classpath.{cmd,sh} and makes all needed jars available to child processes
    // when the assembly is built with the &quot;*-provided&quot; profiles enabled.
    val testingClassPath =
      if (sys.props.contains(&quot;spark.testing&quot;)) {
        sys.props(&quot;java.class.path&quot;).split(java.io.File.pathSeparator).toSeq
      } else {
        Nil
      }
    // Start executors with a few necessary configs for registering with the scheduler
    val sparkJavaOpts = Utils.sparkJavaOpts(conf, SparkConf.isExecutorStartupConf)
    val javaOpts = sparkJavaOpts ++ extraJavaOpts
    val command = Command(&quot;org.apache.spark.executor.CoarseGrainedExecutorBackend&quot;,
      args, sc.executorEnvs, classPathEntries ++ testingClassPath, libraryPathEntries, javaOpts)
    val appUIAddress = sc.ui.map(_.appUIAddress).getOrElse(&quot;&quot;)
    val coresPerExecutor = conf.getOption(&quot;spark.executor.cores&quot;).map(_.toInt)
    val appDesc = new ApplicationDescription(sc.appName, maxCores, sc.executorMemory,
      command, appUIAddress, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor)
    client = new AppClient(sc.env.rpcEnv, masters, appDesc, this, conf)
    client.start()
    waitForRegistration()
  }
</code></pre><p>首先会调用父类的 start 方法</p><pre><code>override def start() {
  val properties = new ArrayBuffer[(String, String)]
  for ((key, value) &lt;- scheduler.sc.conf.getAll) {
    if (key.startsWith(&quot;spark.&quot;)) {
      properties += ((key, value))
    }
  }
  // TODO (prashant) send conf instead of properties
  driverEndpoint = rpcEnv.setupEndpoint(
    CoarseGrainedSchedulerBackend.ENDPOINT_NAME, new DriverEndpoint(rpcEnv, properties))
}
</code></pre><p>主要是创建了 driver 的代理对象，可以给 driver 发送消息的对象。回到上面的 start 方法，主要是</p><pre><code>client = new AppClient(sc.env.rpcEnv, masters, appDesc, this, conf)
client.start()
</code></pre><p>直接找到这个 start 方法</p><pre><code>def start() {
    // Just launch an rpcEndpoint; it will call back into the listener.
    endpoint = rpcEnv.setupEndpoint(&quot;AppClient&quot;, new ClientEndpoint(rpcEnv))
  }
</code></pre><p>就是创建了一个代理对象，看下这个代理对象</p><pre><code>private class ClientEndpoint(override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint
    with Logging {
    private var master: Option[RpcEndpointRef] = None
    // To avoid calling listener.disconnected() multiple times
    private var alreadyDisconnected = false
    @volatile private var alreadyDead = false // To avoid calling listener.dead() multiple times
    @volatile private var registerMasterFutures: Array[JFuture[_]] = null
    @volatile private var registrationRetryTimer: JScheduledFuture[_] = null
    // A thread pool for registering with masters. Because registering with a master is a blocking
    // action, this thread pool must be able to create &quot;masterRpcAddresses.size&quot; threads at the same
    // time so that we can register with all masters.
    private val registerMasterThreadPool = ThreadUtils.newDaemonCachedThreadPool(
      &quot;appclient-register-master-threadpool&quot;,
      masterRpcAddresses.length // Make sure we can register with all masters at the same time
    )
    // A scheduled executor for scheduling the registration actions
    private val registrationRetryThread =
      ThreadUtils.newDaemonSingleThreadScheduledExecutor(&quot;appclient-registration-retry-thread&quot;)
    override def onStart(): Unit = {
      try {
        registerWithMaster(1)
      } catch {
        case e: Exception =&gt;
          logWarning(&quot;Failed to connect to master&quot;, e)
          markDisconnected()
          stop()
      }
    }
    /**
     *  Register with all masters asynchronously and returns an array `Future`s for cancellation.
     */
    private def tryRegisterAllMasters(): Array[JFuture[_]] = {
      for (masterAddress &lt;- masterRpcAddresses) yield {
        registerMasterThreadPool.submit(new Runnable {
          override def run(): Unit = try {
            if (registered) {
              return
            }
            logInfo(&quot;Connecting to master &quot; + masterAddress.toSparkURL + &quot;...&quot;)
            val masterRef =
              rpcEnv.setupEndpointRef(Master.SYSTEM_NAME, masterAddress, Master.ENDPOINT_NAME)
            masterRef.send(RegisterApplication(appDescription, self))
          } catch {
            case ie: InterruptedException =&gt; // Cancelled
            case NonFatal(e) =&gt; logWarning(s&quot;Failed to connect to master $masterAddress&quot;, e)
          }
        })
      }
    }
    /**
     * Register with all masters asynchronously. It will call `registerWithMaster` every
     * REGISTRATION_TIMEOUT_SECONDS seconds until exceeding REGISTRATION_RETRIES times.
     * Once we connect to a master successfully, all scheduling work and Futures will be cancelled.
     *
     * nthRetry means this is the nth attempt to register with master.
     */
    private def registerWithMaster(nthRetry: Int) {
      registerMasterFutures = tryRegisterAllMasters()
      registrationRetryTimer = registrationRetryThread.scheduleAtFixedRate(new Runnable {
        override def run(): Unit = {
          Utils.tryOrExit {
            if (registered) {
              registerMasterFutures.foreach(_.cancel(true))
              registerMasterThreadPool.shutdownNow()
            } else if (nthRetry &gt;= REGISTRATION_RETRIES) {
              markDead(&quot;All masters are unresponsive! Giving up.&quot;)
            } else {
              registerMasterFutures.foreach(_.cancel(true))
              registerWithMaster(nthRetry + 1)
            }
          }
        }
      }, REGISTRATION_TIMEOUT_SECONDS, REGISTRATION_TIMEOUT_SECONDS, TimeUnit.SECONDS)
    }
    /**
     * Send a message to the current master. If we have not yet registered successfully with any
     * master, the message will be dropped.
     */
    private def sendToMaster(message: Any): Unit = {
      master match {
        case Some(masterRef) =&gt; masterRef.send(message)
        case None =&gt; logWarning(s&quot;Drop $message because has not yet connected to master&quot;)
      }
    }
    private def isPossibleMaster(remoteAddress: RpcAddress): Boolean = {
      masterRpcAddresses.contains(remoteAddress)
    }
    override def receive: PartialFunction[Any, Unit] = {
      case RegisteredApplication(appId_, masterRef) =&gt;
        // FIXME How to handle the following cases?
        // 1. A master receives multiple registrations and sends back multiple
        // RegisteredApplications due to an unstable network.
        // 2. Receive multiple RegisteredApplication from different masters because the master is
        // changing.
        appId = appId_
        registered = true
        master = Some(masterRef)
        listener.connected(appId)
      case ApplicationRemoved(message) =&gt;
        markDead(&quot;Master removed our application: %s&quot;.format(message))
        stop()
      case ExecutorAdded(id: Int, workerId: String, hostPort: String, cores: Int, memory: Int) =&gt;
        val fullId = appId + &quot;/&quot; + id
        logInfo(&quot;Executor added: %s on %s (%s) with %d cores&quot;.format(fullId, workerId, hostPort,
          cores))
        // FIXME if changing master and `ExecutorAdded` happen at the same time (the order is not
        // guaranteed), `ExecutorStateChanged` may be sent to a dead master.
        sendToMaster(ExecutorStateChanged(appId, id, ExecutorState.RUNNING, None, None))
        listener.executorAdded(fullId, workerId, hostPort, cores, memory)
      case ExecutorUpdated(id, state, message, exitStatus) =&gt;
        val fullId = appId + &quot;/&quot; + id
        val messageText = message.map(s =&gt; &quot; (&quot; + s + &quot;)&quot;).getOrElse(&quot;&quot;)
        logInfo(&quot;Executor updated: %s is now %s%s&quot;.format(fullId, state, messageText))
        if (ExecutorState.isFinished(state)) {
          listener.executorRemoved(fullId, message.getOrElse(&quot;&quot;), exitStatus)
        }
      case MasterChanged(masterRef, masterWebUiUrl) =&gt;
        logInfo(&quot;Master has changed, new master is at &quot; + masterRef.address.toSparkURL)
        master = Some(masterRef)
        alreadyDisconnected = false
        masterRef.send(MasterChangeAcknowledged(appId))
    }
    override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {
      case StopAppClient =&gt;
        markDead(&quot;Application has been stopped.&quot;)
        sendToMaster(UnregisterApplication(appId))
        context.reply(true)
        stop()
      case r: RequestExecutors =&gt;
        master match {
          case Some(m) =&gt; context.reply(m.askWithRetry[Boolean](r))
          case None =&gt;
            logWarning(&quot;Attempted to request executors before registering with Master.&quot;)
            context.reply(false)
        }
      case k: KillExecutors =&gt;
        master match {
          case Some(m) =&gt; context.reply(m.askWithRetry[Boolean](k))
          case None =&gt;
            logWarning(&quot;Attempted to kill executors before registering with Master.&quot;)
            context.reply(false)
        }
    }
    override def onDisconnected(address: RpcAddress): Unit = {
      if (master.exists(_.address == address)) {
        logWarning(s&quot;Connection to $address failed; waiting for master to reconnect...&quot;)
        markDisconnected()
      }
    }
    override def onNetworkError(cause: Throwable, address: RpcAddress): Unit = {
      if (isPossibleMaster(address)) {
        logWarning(s&quot;Could not connect to $address: $cause&quot;)
      }
    }
    /**
     * Notify the listener that we disconnected, if we hadn&apos;t already done so before.
     */
    def markDisconnected() {
      if (!alreadyDisconnected) {
        listener.disconnected()
        alreadyDisconnected = true
      }
    }
    def markDead(reason: String) {
      if (!alreadyDead) {
        listener.dead(reason)
        alreadyDead = true
      }
    }
    override def onStop(): Unit = {
      if (registrationRetryTimer != null) {
        registrationRetryTimer.cancel(true)
      }
      registrationRetryThread.shutdownNow()
      registerMasterFutures.foreach(_.cancel(true))
      registerMasterThreadPool.shutdownNow()
    }
  }
</code></pre><p>因为继承了 ThreadSafeRpcEndpoint 这个类，也就会依次调用 onstart， receive 等方法，类似与上一遍博客中 akka 的生命周期</p><pre><code>private[spark] trait ThreadSafeRpcEndpoint extends RpcEndpoint
/**
 * An end point for the RPC that defines what functions to trigger given a message.
 *
 * It is guaranteed that `onStart`, `receive` and `onStop` will be called in sequence.
 *
 * The life-cycle of an endpoint is:
 *
 * constructor -&gt; onStart -&gt; receive* -&gt; onStop
 *
 * Note: `receive` can be called concurrently. If you want `receive` to be thread-safe, please use
 * [[ThreadSafeRpcEndpoint]]
 *
 * If any error is thrown from one of [[RpcEndpoint]] methods except `onError`, `onError` will be
 * invoked with the cause. If `onError` throws an error, [[RpcEnv]] will ignore it.
</code></pre><p>那么会首先调用</p><pre><code>override def onStart(): Unit = {
      try {

        registerWithMaster(1)
      } catch {
        case e: Exception =&gt;
          logWarning(&quot;Failed to connect to master&quot;, e)
          markDisconnected()
          stop()
      }
    }
</code></pre><p>直接看 registerWithMaster 方法，像 master 注册我们之前封装好的 application</p><pre><code>/**
    *  Register with all masters asynchronously and returns an array `Future`s for cancellation.
    */
   private def tryRegisterAllMasters(): Array[JFuture[_]] = {
     for (masterAddress &lt;- masterRpcAddresses) yield {
       registerMasterThreadPool.submit(new Runnable {
         override def run(): Unit = try {
           if (registered) {
             return
           }
           logInfo(&quot;Connecting to master &quot; + masterAddress.toSparkURL + &quot;...&quot;)
           val masterRef =
             rpcEnv.setupEndpointRef(Master.SYSTEM_NAME, masterAddress, Master.ENDPOINT_NAME)
           masterRef.send(RegisterApplication(appDescription, self))
         } catch {
           case ie: InterruptedException =&gt; // Cancelled
           case NonFatal(e) =&gt; logWarning(s&quot;Failed to connect to master $masterAddress&quot;, e)
         }
       })
     }
   }
</code></pre><p>会调用 master 的代理对象，然后调用 send 方法，send 方法实际就是底层调用 akka 方法，这里我们可以先看下，找到这个 AkkaRpcEnv 这个类，看下面的方法，也就是上面调用的 setupEndpointRef 方法</p><pre><code>override def setupEndpoint(name: String, endpoint: RpcEndpoint): RpcEndpointRef = {
    @volatile var endpointRef: AkkaRpcEndpointRef = null
    // Use lazy because the Actor needs to use `endpointRef`.
    // So `actorRef` should be created after assigning `endpointRef`.
    lazy val actorRef = actorSystem.actorOf(Props(new Actor with ActorLogReceive with Logging {
      assert(endpointRef != null)
      override def preStart(): Unit = {
        // Listen for remote client network events
        context.system.eventStream.subscribe(self, classOf[AssociationEvent])
        safelyCall(endpoint) {
          endpoint.onStart()
        }
      }
      override def receiveWithLogging: Receive = {
        case AssociatedEvent(_, remoteAddress, _) =&gt;
          safelyCall(endpoint) {
            endpoint.onConnected(akkaAddressToRpcAddress(remoteAddress))
          }
        case DisassociatedEvent(_, remoteAddress, _) =&gt;
          safelyCall(endpoint) {
            endpoint.onDisconnected(akkaAddressToRpcAddress(remoteAddress))
          }
        case AssociationErrorEvent(cause, localAddress, remoteAddress, inbound, _) =&gt;
          safelyCall(endpoint) {
            endpoint.onNetworkError(cause, akkaAddressToRpcAddress(remoteAddress))
          }
        case e: AssociationEvent =&gt;
          // TODO ignore?
        case m: AkkaMessage =&gt;
          logDebug(s&quot;Received RPC message: $m&quot;)
          safelyCall(endpoint) {
            processMessage(endpoint, m, sender)
          }
        case AkkaFailure(e) =&gt;
          safelyCall(endpoint) {
            throw e
          }
        case message: Any =&gt; {
          logWarning(s&quot;Unknown message: $message&quot;)
        }
      }
      override def postStop(): Unit = {
        unregisterEndpoint(endpoint.self)
        safelyCall(endpoint) {
          endpoint.onStop()
        }
      }
      }), name = name)
    endpointRef = new AkkaRpcEndpointRef(defaultAddress, actorRef, conf, initInConstructor = false)
    registerEndpoint(endpoint, endpointRef)
    // Now actorRef can be created safely
    endpointRef.init()
    endp
</code></pre><p>注意最后会返回 AkkaRpcEndpointRef 对象，而这个对象重写了 send 方法</p><pre><code>override def send(message: Any): Unit = {
   actorRef ! AkkaMessage(message, false)
 }
</code></pre><p>也就是 akka 的方法了，很显然。我们回到最初的 sparkcontext</p><pre><code>// Create and start the scheduler
    val (sched, ts) = SparkContext.createTaskScheduler(this, master)
    _schedulerBackend = sched
    _taskScheduler = ts
    _dagScheduler = new DAGScheduler(this)
    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
    // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&apos;s
    // constructor
    _taskScheduler.start()
</code></pre><p>其实这个 start 的方法被调用时，backend 才会被启动。才会去 master 注册 application</p><p>这里就分析完了，driver 向 master 注册 application。下篇博客会继续往后分析注册完后，对资源进行调度，然后分配 executor.</p></div><div style="width:300px;height:250px;float:left"><ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196" data-ad-slot="4142158067"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div style="width:300px;height:250px;float:left"><ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196" data-ad-slot="5618891265"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/协同过滤推荐算法详解.html" rel="next" title="协同过滤推荐算法详解 » java面试题"><i class="fa fa-chevron-left"></i> 协同过滤推荐算法详解 » java面试题</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/weakhashmap整理.html" rel="prev" title="WeakHashMap整理">WeakHashMap整理 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Hello</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1358</span> <span class="site-state-item-name">posts</span></a></div></nav></div></section></div></aside><div class="sfix"><div class="fixedme"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9477174171188196" data-ad-slot="9597600460" data-ad-format="auto"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></div></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Hello</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script></body></html><!-- rebuild by neat -->