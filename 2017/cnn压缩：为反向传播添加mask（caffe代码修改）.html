<!-- build time:Sat Oct 27 2018 21:00:23 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next muse use-motion"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="description" content="1void InnerProductLayer::LayerSetUp(const vector&amp;lt;Blob&amp;gt;&amp;amp; bottom,2const vector&amp;lt;Blob&amp;gt;&amp;amp; top) {3 …4this-&amp;gt;blobs_[0].reset(new Blob(weight_shape));5this-&amp;gt;blobs_[0]-&amp;gt;Addmask();6 …"><meta name="keywords" content="JAVA,面试"><meta property="og:type" content="article"><meta property="og:title" content="CNN压缩：为反向传播添加mask（caffe代码修改）"><meta property="og:url" content="http://www.jfox.info/2017/cnn压缩：为反向传播添加mask（caffe代码修改）.html"><meta property="og:site_name" content="java面试"><meta property="og:description" content="1void InnerProductLayer::LayerSetUp(const vector&amp;lt;Blob&amp;gt;&amp;amp; bottom,2const vector&amp;lt;Blob&amp;gt;&amp;amp; top) {3 …4this-&amp;gt;blobs_[0].reset(new Blob(weight_shape));5this-&amp;gt;blobs_[0]-&amp;gt;Addmask();6 …"><meta property="og:locale" content="zh_CN"><meta property="og:updated_time" content="2018-10-27T12:29:37.379Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CNN压缩：为反向传播添加mask（caffe代码修改）"><meta name="twitter:description" content="1void InnerProductLayer::LayerSetUp(const vector&amp;lt;Blob&amp;gt;&amp;amp; bottom,2const vector&amp;lt;Blob&amp;gt;&amp;amp; top) {3 …4this-&amp;gt;blobs_[0].reset(new Blob(weight_shape));5this-&amp;gt;blobs_[0]-&amp;gt;Addmask();6 …"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Muse",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.jfox.info/2017/cnn压缩：为反向传播添加mask（caffe代码修改）.html"><title>CNN压缩：为反向传播添加mask（caffe代码修改） | java面试</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh_CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">java面试</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.jfox.info/2017/cnn压缩：为反向传播添加mask（caffe代码修改）.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Hello"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="java面试"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">CNN压缩：为反向传播添加mask（caffe代码修改）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-01T23:51:12+08:00">2017-01-01</time></span></div></header><div class="post-body" itemprop="articleBody"><p>1void InnerProductLayer<dtype>::LayerSetUp(const vector&lt;Blob<dtype><em>&gt;&amp; bottom,<br>2const vector&lt;Blob<dtype></dtype></em>&gt;&amp; top) {<br>3 …<br>4this-&gt;blobs_[0].reset(new Blob<dtype>(weight_shape));<br>5this-&gt;blobs_[0]-&gt;Addmask();<br>6 …}</dtype></dtype></dtype></p><p>base_conv.cpp:</p><pre><code>1 template &lt;typename Dtype&gt;
2void BaseConvolutionLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
3const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
4    ...
5this-&gt;blobs_[0].reset(new Blob&lt;Dtype&gt;(weight_shape));
6this-&gt;blobs_[0]-&gt;Addmask();
7     ...}
</code></pre><p>修改blob.hpp和blob.cpp，添加成员mask_和相关的方法，在[1]文章的评论里作者已给出源代码。</p><p>[2]中使用layer结构定义mask，layer是相当于数据的一系列操作，或者说是blob的组合方法。</p><p>但是，想要实现在gpu上的操作，数据需要有gpu有关的操作。故此处采用[1]中的方法，<strong>将mask_添加到blob class中，实现mask_属性</strong>。</p><p><strong>mask的初始化？</strong></p><p>在Caffe框架下，网络的初始化有两种方式，一种是调用filler，按照模型中定义的初始化方式进行初始化，第二种是从已有的caffemodel或者snapshot中读取相应参数矩阵进行初始化[1]。</p><p>1、filler的方法</p><p>在程序开始时，网络使用net.cpp中的Init()进行初始化，由输入至输出，依次调用各个层的layersetup，建立网络结构。如下所示是caffe中使用xavier方法进行填充的操作。</p><pre><code> 1virtualvoid Fill(Blob&lt;Dtype&gt;* blob) {
 2     CHECK(blob-&gt;count());
 3int fan_in = blob-&gt;count() / blob-&gt;num();
 4int fan_out = blob-&gt;count() / blob-&gt;channels();
 5     Dtype n = fan_in;  // default to fan_in 6if (this-&gt;filler_param_.variance_norm() ==
 7        FillerParameter_VarianceNorm_AVERAGE) {
 8       n = (fan_in + fan_out) / Dtype(2);
 9     } elseif (this-&gt;filler_param_.variance_norm() ==
10        FillerParameter_VarianceNorm_FAN_OUT) {
11       n = fan_out;
12    }
13     Dtype scale = sqrt(Dtype(3) / n);
14     caffe_rng_uniform&lt;Dtype&gt;(blob-&gt;count(), -scale, scale,
15         blob-&gt;mutable_cpu_data());
16//Filler&lt;Dtype&gt;:: FillMask(blob);17     CHECK_EQ(this-&gt;filler_param_.sparse(), -1)
18          &lt;&lt; &quot;Sparsity not supported by this Filler.&quot;;
19   }
</code></pre><p>filler的作用是，为建立的网络结构产生随机初始化值。</p><p>即使是从snapshot或caffemodel中读入数据，也执行随机填充操作。</p><p>2、从snapshot或caffemodel中读入数据</p><p>tools/caffe.cpp 中的phase:train可以从snapshot或caffemodel中提取参数，进行finetune。phase:test则可以从提取的参数中建立网络，进行预测过程。</p><p>这里笔者的网络结构是在pycaffe中进行稀疏化的，因此读入网络的proto文件是一个连接数不变、存在部分连接权值为零的网络。需要在读入参数的同时初始化mask_。因此修改blob.cpp中的fromproto函数：</p><pre><code> 1 template &lt;typename Dtype&gt;
 2void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {
 3if (reshape) {
 4     vector&lt;int&gt; shape;
 5if (proto.has_num() || proto.has_channels() ||
 6         proto.has_height() || proto.has_width()) {
 7// Using deprecated 4D Blob dimensions --
 8// shape is (num, channels, height, width). 9       shape.resize(4);
10       shape[0] = proto.num();
11       shape[1] = proto.channels();
12       shape[2] = proto.height();
13       shape[3] = proto.width();
14     } else {
15      shape.resize(proto.shape().dim_size());
16for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {
17         shape[i] = proto.shape().dim(i);
18      }
19    }
20    Reshape(shape);
21   } else {
22     CHECK(ShapeEquals(proto)) &lt;&lt; &quot;shape mismatch (reshape not set)&quot;;
23  }
24// copy data25   Dtype* data_vec = mutable_cpu_data();
26if (proto.double_data_size() &gt; 0) {
27    CHECK_EQ(count_, proto.double_data_size());
28for (int i = 0; i &lt; count_; ++i) {
29       data_vec[i] = proto.double_data(i);
30    }
31   } else {
32    CHECK_EQ(count_, proto.data_size());
33for (int i = 0; i &lt; count_; ++i) {
34       data_vec[i] = proto.data(i);
35    }
36  }
37if (proto.double_diff_size() &gt; 0) {
38    CHECK_EQ(count_, proto.double_diff_size());
39     Dtype* diff_vec = mutable_cpu_diff();
40for (int i = 0; i &lt; count_; ++i) {
41       diff_vec[i] = proto.double_diff(i);
42    }
43   } elseif (proto.diff_size() &gt; 0) {
44    CHECK_EQ(count_, proto.diff_size());
45     Dtype* diff_vec = mutable_cpu_diff();
46for (int i = 0; i &lt; count_; ++i) {
47       diff_vec[i] = proto.diff(i);
48    }
49  }
50if(shape_.size()==4||shape_.size()==2){
51     Dtype* mask_vec = mutable_cpu_data();
52    CHECK(count_);
53for(int i=0;i&lt;count_;i++)
54       mask_vec[i]=data_vec[i]?1:0;
55 }
</code></pre><p>在读入proto文件的同时，如果层的大小是4D——conv层、或2D——fc层时，初始化mask_为data_vec[i]?1:0。当层的大小是1Ds——pool或relu层时，不进行mask的初始化。</p><p><strong>反向传播的修改？</strong></p><p>1、修改blob的更新方式，添加math_funcion.hpp头文件。</p><pre><code> 1 template &lt;typename Dtype&gt;
 2void Blob&lt;Dtype&gt;::Update() {
 3// We will perform update based on where the data is located. 4switch (data_-&gt;head()) {
 5case SyncedMemory::HEAD_AT_CPU:
 6// perform computation on CPU 7     caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),
 8         static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),
 9         static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
10     caffe_mul&lt;Dtype&gt;(count_,
11       static_cast&lt;const Dtype*&gt;(mask_-&gt;cpu_data()),
12       static_cast&lt;const Dtype*&gt;(data_-&gt;cpu_data()),
13       static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
14break;
15case SyncedMemory::HEAD_AT_GPU:
16case SyncedMemory::SYNCED:
17#ifndef CPU_ONLY
18// perform computation on GPU19     caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),
20         static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),
21         static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
22     caffe_gpu_mul&lt;Dtype&gt;(count_,
23       static_cast&lt;const Dtype*&gt;(mask_-&gt;gpu_data()),
24       static_cast&lt;const Dtype*&gt;(data_-&gt;gpu_data()),
25       static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
26#else27    NO_GPU;
28#endif29break;
30default:
31     LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;
32  }
33 }
</code></pre><p>2、为cpu下的计算和gpu下的计算分别添加形如weight[i]*=mask[i];的运算方式。</p><p>inner_product_layer.cpp:</p><pre><code> 1void InnerProductLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
 2const vector&lt;bool&gt;&amp; propagate_down,
 3const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
 4if (this-&gt;param_propagate_down_[0]) {
 5const Dtype* top_diff = top[0]-&gt;cpu_diff();
 6const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
 7// Gradient with respect to weight 8     Dtype* weight_diff = this-&gt;blobs_[0]-&gt;mutable_cpu_diff();
 9     vector&lt;int&gt; weight_shape(2);
10if (transpose_) {
11       weight_shape[0] = K_;
12       weight_shape[1] = N_;
13     } else {
14       weight_shape[0] = N_;
15       weight_shape[1] = K_;
16    }
17int count = weight_shape[0]*weight_shape[1];
18const Dtype* mask = this-&gt;blobs_[0]-&gt;cpu_mask();
19for(int j=0;j&lt;count;j++)
20       weight_diff[j]*=mask[j];
2122if (transpose_) {
23       caffe_cpu_gemm&lt;Dtype&gt;(CblasTrans, CblasNoTrans,
24          K_, N_, M_,
25           (Dtype)1., bottom_data, top_diff,
26           (Dtype)1., weight_diff);
27     } else {
28       caffe_cpu_gemm&lt;Dtype&gt;(CblasTrans, CblasNoTrans,
29          N_, K_, M_,
30           (Dtype)1., top_diff, bottom_data,
31           (Dtype)1., weight_diff);
32    }
33  }
34if (bias_term_ &amp;&amp; this-&gt;param_propagate_down_[1]) {
35const Dtype* top_diff = top[0]-&gt;cpu_diff();
36// Gradient with respect to bias37     caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, M_, N_, (Dtype)1., top_diff,
38         bias_multiplier_.cpu_data(), (Dtype)1.,
39this-&gt;blobs_[1]-&gt;mutable_cpu_diff());
40  }
41if (propagate_down[0]) {
42const Dtype* top_diff = top[0]-&gt;cpu_diff();
43// Gradient with respect to bottom data44if (transpose_) {
45       caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasTrans,
46          M_, K_, N_,
47           (Dtype)1., top_diff, this-&gt;blobs_[0]-&gt;cpu_data(),
48           (Dtype)0., bottom[0]-&gt;mutable_cpu_diff());
49     } else {
50       caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans,
51          M_, K_, N_,
52           (Dtype)1., top_diff, this-&gt;blobs_[0]-&gt;cpu_data(),
53           (Dtype)0., bottom[0]-&gt;mutable_cpu_diff());
54    }
55  }
56 }
</code></pre><p>inner_product_layer.cu:</p><pre><code> 1 template &lt;typename Dtype&gt;
 2void InnerProductLayer&lt;Dtype&gt;::Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
 3const vector&lt;bool&gt;&amp; propagate_down,
 4const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
 5if (this-&gt;param_propagate_down_[0]) {
 6const Dtype* top_diff = top[0]-&gt;gpu_diff();
 7const Dtype* bottom_data = bottom[0]-&gt;gpu_data();
 8     vector&lt;int&gt; weight_shape(2);
 9if (transpose_) {
10       weight_shape[0] = K_;
11       weight_shape[1] = N_;
12     } else {
13       weight_shape[0] = N_;
14       weight_shape[1] = K_;
15    }
16int count = weight_shape[0]*weight_shape[1];
17     caffe_gpu_mul&lt;Dtype&gt;(count,static_cast&lt;const Dtype*&gt;(this-&gt;blobs_[0]-&gt;mutable_gpu_diff()),static_cast&lt;const Dtype*&gt;(this-&gt;blobs_[0]-&gt;gpu_mask()),static_cast&lt;Dtype*&gt;(this-&gt;blobs_[0]-&gt;mutable_gpu_diff()));
18     Dtype* weight_diff = this-&gt;blobs_[0]-&gt;mutable_gpu_diff();
19//for(int j=0;j&lt;count;j++)
20//weight_diff[j]*=this-&gt;masks_[j];
21// Gradient with respect to weight22if (transpose_) {
23       caffe_gpu_gemm&lt;Dtype&gt;(CblasTrans, CblasNoTrans,
24          K_, N_, M_,
25           (Dtype)1., bottom_data, top_diff,
26           (Dtype)1., weight_diff);
27     } else {
28       caffe_gpu_gemm&lt;Dtype&gt;(CblasTrans, CblasNoTrans,
29          N_, K_, M_,
30           (Dtype)1., top_diff, bottom_data,
31           (Dtype)1., weight_diff);
32    }
33  }
34if (bias_term_ &amp;&amp; this-&gt;param_propagate_down_[1]) {
35const Dtype* top_diff = top[0]-&gt;gpu_diff();
36// Gradient with respect to bias37     caffe_gpu_gemv&lt;Dtype&gt;(CblasTrans, M_, N_, (Dtype)1., top_diff,
38         bias_multiplier_.gpu_data(), (Dtype)1.,
39this-&gt;blobs_[1]-&gt;mutable_gpu_diff());
40  }
41if (propagate_down[0]) {
42const Dtype* top_diff = top[0]-&gt;gpu_diff();
43// Gradient with respect to bottom data44if (transpose_) {
45       caffe_gpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasTrans,
46          M_, K_, N_,
47           (Dtype)1., top_diff, this-&gt;blobs_[0]-&gt;gpu_data(),
48           (Dtype)0., bottom[0]-&gt;mutable_gpu_diff());
49     } else {
50       caffe_gpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans,
51          M_, K_, N_,
52          (Dtype)1., top_diff, this-&gt;blobs_[0]-&gt;gpu_data(),
53          (Dtype)0., bottom[0]-&gt;mutable_gpu_diff());
54    }
55  }
56 }
</code></pre><p>至此修改完毕。</p><p>另外，caffe在新的版本中已添加sparse_参数，参考 <a href="https://github.com/BVLC/caffe/pulls?utf8=%E2%9C%93&amp;q=sparse" target="_blank" rel="noopener">https://github.com/BVLC/caffe/pulls?utf8=%E2%9C%93&amp;q=sparse</a></p></div><div style="width:300px;height:250px;float:left"><ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196" data-ad-slot="4142158067"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><div style="width:300px;height:250px;float:left"><ins class="adsbygoogle" style="display:inline-block;width:300px;height:250px" data-ad-client="ca-pub-9477174171188196" data-ad-slot="5618891265"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/坚持再坚持-技术总结-分享项目源码-总结3.html" rel="next" title="坚持,再坚持—技术总结 分享项目源码—总结3"><i class="fa fa-chevron-left"></i> 坚持,再坚持—技术总结 分享项目源码—总结3</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/神奇的深度图-复杂的效果-不复杂的原理.html" rel="prev" title="神奇的深度图：复杂的效果，不复杂的原理">神奇的深度图：复杂的效果，不复杂的原理 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Hello</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">1358</span> <span class="site-state-item-name">posts</span></a></div></nav></div></section></div></aside><div class="sfix"><div class="fixedme"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9477174171188196" data-ad-slot="9597600460" data-ad-format="auto"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></div></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Hello</span></div><div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div><span class="post-meta-divider">|</span><div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script></body></html><!-- rebuild by neat -->